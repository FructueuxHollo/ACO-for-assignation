{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb945470-1db4-4215-b34b-2545f84cb404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from copy import deepcopy\n",
    "from threading import Thread\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "816ee1d0-8a5a-4ba9-9293-f86b14e9b3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPU:\n",
    "    def __init__(self, number_of_cores=0, clock_rate_in_hz=0.0, family_name='', denomination=''):\n",
    "        \"\"\"\n",
    "        Initializes a new CPU with the given parameters.\n",
    "        \"\"\"\n",
    "        self.number_of_cores = number_of_cores\n",
    "        self.clock_rate_in_hz = clock_rate_in_hz\n",
    "        self.family_name = family_name\n",
    "        self.denomination = denomination\n",
    "\n",
    "    def duplicate(self):\n",
    "        \"\"\"\n",
    "        Creates and returns a deep copy of this CPU.\n",
    "        \"\"\"\n",
    "        return deepcopy(self)\n",
    "\n",
    "    def outperforms(self, cpu_to_compare_with, multithreaded_program_execution):\n",
    "        \"\"\"\n",
    "        Returns True if this CPU outperforms the one passed in as a parameter for the specified type of program.\n",
    "        \"\"\"\n",
    "\n",
    "        # Priority mapping for CPU families and denominations\n",
    "        cpu_hierarchy = {\n",
    "            'core i9': 4,\n",
    "            'core i7': 3,\n",
    "            'core i5': 2,\n",
    "            'core i3': 1\n",
    "        }\n",
    "\n",
    "        # Determine the CPU type\n",
    "        this_cpu_type = f\"{self.family_name} {self.denomination}\".lower()\n",
    "        other_cpu_type = f\"{cpu_to_compare_with.family_name} {cpu_to_compare_with.denomination}\".lower()\n",
    "\n",
    "        # Compare based on family/denomination hierarchy\n",
    "        this_priority = cpu_hierarchy.get(this_cpu_type, 0)\n",
    "        other_priority = cpu_hierarchy.get(other_cpu_type, 0)\n",
    "\n",
    "        if this_priority != other_priority:\n",
    "            return this_priority > other_priority\n",
    "\n",
    "        # If the same family, compare based on multithread or single-thread performance\n",
    "        if multithreaded_program_execution:\n",
    "            return self.number_of_cores > cpu_to_compare_with.number_of_cores\n",
    "        else:\n",
    "            return self.clock_rate_in_hz > cpu_to_compare_with.clock_rate_in_hz\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        \"\"\"\n",
    "        Compares if two CPU objects are equivalent in terms of performance.\n",
    "        This allows using '==' for CPU objects.\n",
    "        \"\"\"\n",
    "        if not isinstance(other, CPU):\n",
    "            return NotImplemented\n",
    "        return (self.family_name == other.family_name and\n",
    "                self.denomination == other.denomination and\n",
    "                self.number_of_cores == other.number_of_cores and\n",
    "                self.clock_rate_in_hz == other.clock_rate_in_hz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e101d74-8f35-4deb-a672-6927427a3ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker:\n",
    "    def __init__(self, ID=0, cpu_info=None, current_global_cpu_time=0.0, available_memory_size=0.0, \n",
    "                 available_disk_size=0.0, assigned_jobs=None, connection_bandwidth_with_master_pc=0.0, \n",
    "                 connection_delay_with_master_pc=0.0, cpu_usage_in_percentage=0.0, name=\"\", \n",
    "                 original_available_memory_size=0.0, original_available_disk_size=0.0, \n",
    "                 original_connection_bandwidth_with_master_pc=0.0, original_connection_delay_with_master_pc=0.0):\n",
    "        self.ID = ID\n",
    "        self.cpu_info = cpu_info\n",
    "        self.available_memory_size = available_memory_size\n",
    "        self.available_disk_size = available_disk_size\n",
    "        self.connection_bandwidth_with_master_pc = connection_bandwidth_with_master_pc\n",
    "        self.connection_delay_with_master_pc = connection_delay_with_master_pc\n",
    "        self.original_available_memory_size = original_available_memory_size\n",
    "        self.original_available_disk_size = original_available_disk_size\n",
    "        self.original_connection_bandwidth_with_master_pc = original_connection_bandwidth_with_master_pc\n",
    "        self.original_connection_delay_with_master_pc = original_connection_delay_with_master_pc\n",
    "        self.assigned_jobs = assigned_jobs if assigned_jobs is not None else []\n",
    "        self.cpu_usage_in_percentage = cpu_usage_in_percentage\n",
    "        self.name = name\n",
    "        self.current_global_cpu_time = current_global_cpu_time\n",
    "        self.base10_name = 0\n",
    "    def duplicate(self):\n",
    "        return deepcopy(self)\n",
    "\n",
    "    def print(self):\n",
    "        print(\"********** Worker PC Details **********\")\n",
    "        print(f\"ID: {self.ID}\")\n",
    "        print(f\"Name: {self.name}\")\n",
    "        print(f\"Current Global CPU Time: {self.current_global_cpu_time}\")\n",
    "        print(f\"CPU Number Of Cores: {self.cpu_info.number_of_cores}\")\n",
    "        print(f\"CPU Clock Rate In GHz: {self.cpu_info.clock_rate_in_hz / 1_000_000_000}\")\n",
    "        print(f\"CPU Name: {self.cpu_info.family_name} {self.cpu_info.denomination}\")\n",
    "        print(f\"Available Memory Size: {self.available_memory_size}\")\n",
    "        print(f\"Available Disk Size: {self.available_disk_size}\")\n",
    "        print(f\"Connection Bandwidth With Master PC: {self.connection_bandwidth_with_master_pc}\")\n",
    "        print(f\"Connection Delay With Master PC: {self.connection_delay_with_master_pc}\")\n",
    "        print(f\"CPU Usage In Percentage: {self.cpu_usage_in_percentage}\")\n",
    "        print(\"----- Assigned Jobs -----\")\n",
    "        for job in self.assigned_jobs:\n",
    "            print(f\"Job Name: {job.name}\")\n",
    "        print(\"********** End **********\\n\\n\")\n",
    "\n",
    "    def can_handle_job(self, job):\n",
    "        if self.available_memory_size < job.required_memory_size_for_execution:\n",
    "            return False\n",
    "        if self.available_disk_size < job.required_disk_size_for_execution:\n",
    "            return False\n",
    "        if self.cpu_info.number_of_cores < job.thread_process_count:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def reset_resource_usage(self, job):\n",
    "        self.cpu_usage_in_percentage = 0.0\n",
    "        self.available_disk_size += job.required_disk_size_for_execution\n",
    "        self.available_memory_size += job.required_memory_size_for_execution\n",
    "\n",
    "    def execute_job(self, job):\n",
    "        job_execution_thread = Thread(target=job.run)\n",
    "        job_execution_thread.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64662602-70bc-49b1-b4c6-657ad0e51d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Job(Thread):  # Inherit from threading.Thread to make the class runnable\n",
    "    def __init__(self, ID=0, standard_processing_durations=None, required_memory_size_for_execution=0.0, \n",
    "                 required_disk_size_for_execution=0.0, assigned_worker=None, job_current_cpu_time=0.0, \n",
    "                 docker_file_size=0.0, assignment_time=0, estimated_result_file_size=0.0, \n",
    "                 docker_file_generation_duration_on_master_pc=0.0, currently_being_processed_on_assigned_worker=False, \n",
    "                 thread_process_count=1, currently_assigned_to_worker=False, \n",
    "                 finished_being_processed_on_assigned_worker=False, name=\"\", induced_cpu_usage_increase_percentage=0.0):\n",
    "        super().__init__()\n",
    "        self.ID = ID\n",
    "        self.standard_processing_durations = standard_processing_durations or {}\n",
    "        self.required_memory_size_for_execution = required_memory_size_for_execution\n",
    "        self.required_disk_size_for_execution = required_disk_size_for_execution\n",
    "        self.assigned_worker = assigned_worker\n",
    "        self.job_current_cpu_time = job_current_cpu_time\n",
    "        self.docker_file_size = docker_file_size\n",
    "        self.assignment_time = assignment_time\n",
    "        self.estimated_result_file_size = estimated_result_file_size\n",
    "        self.docker_file_generation_duration_on_master_pc = docker_file_generation_duration_on_master_pc\n",
    "        self.currently_being_processed_on_assigned_worker = currently_being_processed_on_assigned_worker\n",
    "        self.thread_process_count = thread_process_count\n",
    "        self.currently_assigned_to_worker = currently_assigned_to_worker\n",
    "        self.finished_being_processed_on_assigned_worker = finished_being_processed_on_assigned_worker\n",
    "        self.name = name\n",
    "        self.induced_cpu_usage_increase_percentage = induced_cpu_usage_increase_percentage\n",
    "\n",
    "    def duplicate(self, job):\n",
    "        return deepcopy(self)\n",
    "\n",
    "    def run(self):\n",
    "        try:\n",
    "            self.currently_being_processed_on_assigned_worker = True\n",
    "            if self.assigned_worker:\n",
    "                print(f\"{self.assigned_worker.name}---<< {self.name} <<---\")\n",
    "\n",
    "                # Simulating processing time based on the assigned worker's CPU info\n",
    "                family_name = self.assigned_worker.cpu_info['family_name']\n",
    "                denomination = self.assigned_worker.cpu_info['denomination']\n",
    "                cores = self.assigned_worker.cpu_info['number_of_cores']\n",
    "                key = f\"{family_name}-{denomination}-{cores}\"\n",
    "\n",
    "                # Sleep for the standard processing duration\n",
    "                if key in self.standard_processing_durations:\n",
    "                    processing_time = self.standard_processing_durations[key]\n",
    "                    time.sleep(processing_time)\n",
    "\n",
    "                # Processing complete\n",
    "                self.finished_being_processed_on_assigned_worker = True\n",
    "                self.currently_being_processed_on_assigned_worker = False\n",
    "                print(f\"{self.assigned_worker.name}--->> {self.name} >>---\")\n",
    "\n",
    "                # Update worker's resources after processing\n",
    "                self.assigned_worker.assigned_jobs.remove(self)\n",
    "                self.assigned_worker.available_disk_size += self.required_disk_size_for_execution\n",
    "                self.assigned_worker.available_memory_size += self.required_memory_size_for_execution\n",
    "                self.assigned_worker.cpu_usage_in_percentage -= self.induced_cpu_usage_increase_percentage\n",
    "\n",
    "                # Reset job's worker assignment\n",
    "                self.currently_assigned_to_worker = False\n",
    "                self.assigned_worker = None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Job execution failed: {e}\")\n",
    "\n",
    "    def print(self):\n",
    "        print(\"++++++++++ Job Details ++++++++++\")\n",
    "        print(f\"ID: {self.ID}\")\n",
    "        print(f\"Name: {self.name}\")\n",
    "        for cpu, duration in self.standard_processing_durations.items():\n",
    "            print(f\"Standard Processing Duration On {cpu}: {duration}\")\n",
    "        print(f\"Required Memory Size For Execution: {self.required_memory_size_for_execution}\")\n",
    "        print(f\"Required Disk Size For Execution: {self.required_disk_size_for_execution}\")\n",
    "        print(f\"Docker File Size: {self.docker_file_size}\")\n",
    "        print(f\"Arrival Time: {self.assignment_time}\")\n",
    "        print(f\"Thread Process Count: {self.thread_process_count}\")\n",
    "        print(f\"Estimated Result File Size: {self.estimated_result_file_size}\")\n",
    "        print(f\"Docker File Generation Duration On Master PC: {self.docker_file_generation_duration_on_master_pc}\")\n",
    "        print(f\"Current CPU Time: {self.job_current_cpu_time}\")\n",
    "        print(f\"Currently Assigned To Worker: {self.currently_assigned_to_worker}\")\n",
    "        print(f\"Currently Being Processed On Assigned Worker: {self.currently_being_processed_on_assigned_worker}\")\n",
    "        print(f\"Finished Being Processed On Assigned Worker: {self.finished_being_processed_on_assigned_worker}\")\n",
    "        if self.assigned_worker:\n",
    "            print(f\"Assigned Worker: {self.assigned_worker.name}\")\n",
    "        print(\"++++++++++ End ++++++++++\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53d67498-8389-4f6f-8619-ff18ca257c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Match:\n",
    "    def __init__(self, value, pheromone=1):\n",
    "        self.value = value  # value is expected to be a tuple (job, worker)\n",
    "        self.pheromone = pheromone\n",
    "        self.processing_duration = self.calculate_processing_duration()\n",
    "\n",
    "    def calculate_processing_duration(self):\n",
    "        job, worker = self.value\n",
    "        # Use the worker's name (e.g., 'PC1') to get the correct duration from the job's dictionary\n",
    "        return job.standard_processing_durations[worker.name]  # Assume worker has an 'name' or identifier like 'PC1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e06dda2-0719-4a9d-8260-19bebba5611d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ant:\n",
    "    def __init__(self, id, path=[]):\n",
    "        self.id = id\n",
    "        self.path = path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40d92f73-9e31-4f98-8d83-079da32ccc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_paths(ants, jobs, matches, alpha, beta):\n",
    "    for ant in ants:\n",
    "        ant.path = []\n",
    "        for job in jobs:\n",
    "            matching_elements = [match for match in matches if match.value[0] == job]\n",
    "            p = calculate_probabilities(matching_elements, alpha, beta)  # calculate probabilities (worker, probability)\n",
    "            r = random.random()\n",
    "            w = pick_worker(r, p)\n",
    "            ant.path.append((job, w))  # append job-worker pair to the ant's path\n",
    "    return ants\n",
    "\n",
    "def calculate_probabilities(matches, alpha, beta):\n",
    "    \"\"\"\n",
    "    Calculate the probability of selecting each worker for the given job.\n",
    "    \n",
    "    :param matches: List of Match objects where value[0] is the job and value[1] is the worker.\n",
    "    :param alpha: Weight for pheromone importance.\n",
    "    :param beta: Weight for processing duration (distance) importance.\n",
    "    :return: List of tuples (worker, probability).\n",
    "    \"\"\"\n",
    "    desirabilities = []\n",
    "    total_desirability = 0\n",
    "\n",
    "    # Calculate desirability for each match and sum them for normalization\n",
    "    for match in matches:\n",
    "        worker = match.value[1]  # worker is the second item in the tuple (job, worker)\n",
    "        pheromone = match.pheromone\n",
    "        processing_duration = match.processing_duration\n",
    "        proximity = 1 / processing_duration  # Inverse of the distance is the proximity\n",
    "\n",
    "        # Desirability formula: τ_ik^α * η_ik^β\n",
    "        desirability = (pheromone ** alpha) * (proximity ** beta)\n",
    "        desirabilities.append((worker, desirability))\n",
    "        total_desirability += desirability\n",
    "\n",
    "    # Normalize desirabilities to get probabilities\n",
    "    probabilities = [(worker, desirability / total_desirability) for worker, desirability in desirabilities]\n",
    "\n",
    "    return probabilities\n",
    "\n",
    "def pick_worker(random_value, worker_probabilities):\n",
    "    \"\"\"\n",
    "    Select a worker based on the calculated probabilities and a random value.\n",
    "    \n",
    "    :param random_value: Random number between 0 and 1.\n",
    "    :param worker_probabilities: List of tuples (worker, probability).\n",
    "    :return: The selected worker.\n",
    "    \"\"\"\n",
    "    cumulative_prob = 0\n",
    "\n",
    "    for worker, probability in worker_probabilities:\n",
    "        cumulative_prob += probability\n",
    "        if random_value <= cumulative_prob:\n",
    "            return worker  # Return the exact worker when the random value falls within the cumulative probability\n",
    "\n",
    "    return worker_probabilities[-1][0]  # Fallback to return the last worker in case of rounding issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "722a1338-930a-4643-a984-6c0c060a3374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pheromone_update(ants, matches, evap_coeff, Q):\n",
    "    \"\"\"\n",
    "    Updates the pheromone levels on the matches based on the ants' paths.\n",
    "\n",
    "    :param ants: List of ants, where each ant has a path attribute (a list of job-worker tuples).\n",
    "    :param matches: List of Match objects with pheromone levels to be updated.\n",
    "    :param evap_coeff: Coefficient for pheromone evaporation (ρ).\n",
    "    :param Q: Constant value for pheromone deposition.\n",
    "    :return: Updated matches with new pheromone levels.\n",
    "    \"\"\"\n",
    "    # Evaporate pheromones on all matches\n",
    "    for match in matches:\n",
    "        match.pheromone *= evap_coeff  # Apply evaporation to all pheromone values\n",
    "\n",
    "    # Update pheromones based on each ant's path\n",
    "    for ant in ants:\n",
    "        l = max_worker_processing_duration(ant.path)  # Max processing duration across workers\n",
    "        pheromone = Q / l      # Calculate pheromone to deposit (Δτ_ij)\n",
    "\n",
    "        for match in matches:\n",
    "            if match.value in ant.path:  # Check if the job-worker pair was part of the ant's path\n",
    "                match.pheromone += pheromone  # Add pheromone based on the ant's contribution\n",
    "\n",
    "    return matches\n",
    "\n",
    "def max_worker_processing_duration(path):\n",
    "    \"\"\"\n",
    "    Calculates the maximum processing duration for any worker in the ant's path, \n",
    "    considering the worker's ability to run jobs simultaneously based on resource availability.\n",
    "    \n",
    "    :param path: A list of tuples (job, worker) representing the path of an ant.\n",
    "    :return: The maximum processing duration across all workers.\n",
    "    \"\"\"\n",
    "    # Dictionary to store jobs per worker, using worker.name as the key\n",
    "    worker_jobs = {}\n",
    "\n",
    "    # Group jobs by worker\n",
    "    for job, worker in path:\n",
    "        if worker.name not in worker_jobs:\n",
    "            worker_jobs[worker.name] = {'worker': worker, 'jobs': []}  # Store worker object alongside jobs\n",
    "        worker_jobs[worker.name]['jobs'].append(job)\n",
    "\n",
    "    max_duration = 0\n",
    "\n",
    "    # Evaluate the total processing duration per worker\n",
    "    for worker_info in worker_jobs.values():\n",
    "        worker = worker_info['worker']  # Get the worker object\n",
    "        jobs = worker_info['jobs']  # Get the list of jobs\n",
    "        worker_duration = calculate_worker_duration(jobs, worker)\n",
    "        max_duration = max(max_duration, worker_duration)\n",
    "\n",
    "    return max_duration\n",
    "\n",
    "def calculate_worker_duration(jobs, worker):\n",
    "    \"\"\"\n",
    "    Calculates the total duration for a worker considering simultaneous job execution\n",
    "    based on the worker's available resources (memory, disk, CPU cores).\n",
    "    \n",
    "    :param jobs: List of jobs assigned to the worker.\n",
    "    :param worker: The worker object with available resources (memory, disk, cores).\n",
    "    :return: The total time required for the worker to complete all jobs.\n",
    "    \"\"\"\n",
    "    # Sort jobs by their processing duration as a heuristic to allocate resources efficiently\n",
    "    jobs = sorted(jobs, key=lambda job: job.standard_processing_durations[worker.name])\n",
    "\n",
    "    total_time = 0\n",
    "    # Initialize available resources for the worker\n",
    "    available_memory = worker.available_memory_size\n",
    "    available_disk = worker.available_disk_size\n",
    "    available_cores = worker.cpu_info.number_of_cores\n",
    "\n",
    "    # List of jobs that are executing in parallel with their remaining durations\n",
    "    executing_jobs = []\n",
    "\n",
    "    # List to track remaining job processing times\n",
    "    remaining_durations = {job: job.standard_processing_durations[worker.name] for job in jobs}\n",
    "\n",
    "    while jobs or executing_jobs:\n",
    "        # Try to allocate more jobs to be executed in parallel\n",
    "        remaining_jobs = []\n",
    "        for job in jobs:\n",
    "            # Check if the worker can handle this job along with the currently executing jobs\n",
    "            if (available_memory >= job.required_memory_size_for_execution and\n",
    "                available_disk >= job.required_disk_size_for_execution and\n",
    "                available_cores >= job.thread_process_count):\n",
    "                \n",
    "                # If the worker can handle this job, add it to the executing jobs\n",
    "                executing_jobs.append(job)\n",
    "                available_memory -= job.required_memory_size_for_execution\n",
    "                available_disk -= job.required_disk_size_for_execution\n",
    "                available_cores -= job.thread_process_count\n",
    "            else:\n",
    "                # If the worker can't handle it, put it back in the remaining jobs\n",
    "                remaining_jobs.append(job)\n",
    "\n",
    "        # If no jobs are currently executing, we advance time by the duration of the job that finishes next\n",
    "        if executing_jobs:\n",
    "            # Find the job with the shortest remaining processing time in the current executing jobs\n",
    "            min_duration = min(remaining_durations[job] for job in executing_jobs)\n",
    "            total_time += min_duration\n",
    "\n",
    "            # Remove completed jobs and update available resources\n",
    "            completed_jobs = []\n",
    "            for job in executing_jobs:\n",
    "                remaining_durations[job] -= min_duration  # Reduce the remaining duration for this job\n",
    "                if remaining_durations[job] == 0:  # Job has finished\n",
    "                    available_memory += job.required_memory_size_for_execution\n",
    "                    available_disk += job.required_disk_size_for_execution\n",
    "                    available_cores += job.thread_process_count\n",
    "                    completed_jobs.append(job)\n",
    "\n",
    "            # Remove completed jobs from the executing list\n",
    "            executing_jobs = [job for job in executing_jobs if job not in completed_jobs]\n",
    "\n",
    "        # Update the jobs list with the remaining jobs for the next iteration\n",
    "        jobs = remaining_jobs\n",
    "\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4600d4c8-1dce-4a4e-a0f8-a944a2055f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(jobs, matches):\n",
    "    \"\"\"\n",
    "    Evaluates the matches to determine the optimal path with the minimum processing duration.\n",
    "    \n",
    "    :param jobs: List of jobs.\n",
    "    :param matches: List of Match objects (job-worker pairs with pheromone and processing duration).\n",
    "    :return: A tuple containing the optimal path and the total processing duration.\n",
    "             The optimal path is a list of tuples (job, worker), and the total duration is an integer.\n",
    "    \"\"\"\n",
    "    optimal_path = []\n",
    "    worker_jobs = {}  # Dictionary to store jobs assigned to each worker\n",
    "\n",
    "    # Build the optimal path by selecting the match with the highest pheromone level for each job\n",
    "    for job in jobs:\n",
    "        # Filter matches that correspond to the current job\n",
    "        job_matches = [match for match in matches if match.value[0] == job]\n",
    "\n",
    "        # Select the match with the highest pheromone level (most likely assignment)\n",
    "        best_match = max(job_matches, key=lambda m: m.pheromone)\n",
    "\n",
    "        # Add the job-worker pair to the optimal path\n",
    "        worker = best_match.value[1]\n",
    "        optimal_path.append((job, worker))\n",
    "\n",
    "        # Store the job under the worker's list of jobs\n",
    "        if worker not in worker_jobs:\n",
    "            worker_jobs[worker] = []\n",
    "        worker_jobs[worker].append(job)\n",
    "\n",
    "    # Calculate the total processing duration based on the max worker processing duration\n",
    "    total_processing_duration = 0\n",
    "    for worker, assigned_jobs in worker_jobs.items():\n",
    "        worker_duration = calculate_worker_duration(assigned_jobs, worker)\n",
    "        total_processing_duration = max(total_processing_duration, worker_duration)\n",
    "\n",
    "    return optimal_path, total_processing_duration\n",
    "    \n",
    "def format_duration(seconds):\n",
    "    \"\"\"\n",
    "    Converts a duration from seconds to a string in the format of hours, minutes, and seconds.\n",
    "    \n",
    "    :param seconds: The total duration in seconds.\n",
    "    :return: A string representing the duration in 'H hours M minutes S seconds' format.\n",
    "    \"\"\"\n",
    "    hours, remainder = divmod(seconds, 3600)  # Get hours and the remainder seconds\n",
    "    minutes, seconds = divmod(remainder, 60)  # Get minutes and remaining seconds\n",
    "\n",
    "    # Build the formatted string\n",
    "    duration_str = f\"{int(hours)} hours {int(minutes)} minutes {int(seconds)} seconds\"\n",
    "    \n",
    "    return duration_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1db159ab-c99d-4bc0-b0e3-db25c81c4769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ACO(jobs, matches, ants, alpha, beta, evap_coeff, Q, max_iterations=100, tolerance=1e-5, patience=50):\n",
    "    \"\"\"\n",
    "    Runs the Ant Colony Optimization (ACO) algorithm to find the optimal job-worker assignments.\n",
    "    \n",
    "    :param jobs: List of jobs.\n",
    "    :param matches: List of Match objects (job-worker pairs with pheromone and processing duration).\n",
    "    :param ants: List of Ant objects, where each ant has a path.\n",
    "    :param alpha: Pheromone influence factor.\n",
    "    :param beta: Heuristic information influence factor.\n",
    "    :param evap_coeff: Coefficient for pheromone evaporation (ρ).\n",
    "    :param Q: Constant value for pheromone deposition.\n",
    "    :param max_iterations: Maximum number of iterations to run.\n",
    "    :param tolerance: Threshold to stop the algorithm when the change in total duration is small enough.\n",
    "    :param patience: Number of consecutive iterations without significant improvement before stopping.\n",
    "    :return: A tuple containing the optimal path and the total processing duration.\n",
    "    \"\"\"\n",
    "    previous_duration = float('inf')  # Start with an infinitely large duration\n",
    "    iteration = 0\n",
    "    no_improvement_count = 0  # Counter for iterations without significant improvement\n",
    "\n",
    "    while iteration < max_iterations:\n",
    "        # Generate paths for ants based on current pheromone levels\n",
    "        ants = generate_paths(ants, jobs, matches, alpha, beta)\n",
    "\n",
    "        # Update pheromone levels based on the paths taken by the ants\n",
    "        matches = pheromone_update(ants, matches, evap_coeff, Q)\n",
    "\n",
    "        # Evaluate the current best path and its total duration\n",
    "        optimal_path, total_duration = evaluate(jobs, matches)\n",
    "\n",
    "        # Calculate the difference in total duration between iterations\n",
    "        duration_difference = abs(previous_duration - total_duration)\n",
    "\n",
    "        # Check if the difference is below the tolerance (convergence criteria)\n",
    "        if duration_difference < tolerance:\n",
    "            no_improvement_count += 1\n",
    "        else:\n",
    "            no_improvement_count = 0  # Reset the count if there's significant improvement\n",
    "\n",
    "        # If there has been no significant improvement for a certain number of iterations, break\n",
    "        if no_improvement_count >= patience:\n",
    "            print(f\"No significant improvement for {patience} consecutive iterations. Stopping.\")\n",
    "            print(f\"Converged at iteration {iteration} with duration difference {duration_difference}\")\n",
    "            break\n",
    "\n",
    "        # If the difference is below the tolerance and we have met the patience criteria, stop\n",
    "        # if no_improvement_count == 0 and duration_difference < tolerance:\n",
    "        #     print(f\"Converged at iteration {iteration} with duration difference {duration_difference}\")\n",
    "        #     break\n",
    "\n",
    "        # Update previous duration and increment iteration counter\n",
    "        previous_duration = total_duration\n",
    "        iteration += 1\n",
    "\n",
    "    # Return the best path found and its total processing duration\n",
    "    return optimal_path, total_duration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2f316a00-c4b0-414c-9530-9bf0adb89e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobData = pd.read_csv(\"data/jobs3.csv\")\n",
    "workersData = pd.read_csv(\"data/workers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fba0ed0d-5fd7-44db-b13f-d99de404a793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Name</th>\n",
       "      <th>Standard Processing Duration PC1</th>\n",
       "      <th>Standard Processing Duration PC2</th>\n",
       "      <th>Standard Processing Duration PC3</th>\n",
       "      <th>Standard Processing Duration PC4</th>\n",
       "      <th>Standard Processing Duration PC5</th>\n",
       "      <th>Standard Processing Duration PC6</th>\n",
       "      <th>Required Memory Size For Execution</th>\n",
       "      <th>Required Disk Size For Execution</th>\n",
       "      <th>Docker File Size</th>\n",
       "      <th>Docker File Generation Duration On Master PC</th>\n",
       "      <th>Estimated Result File Size</th>\n",
       "      <th>Thread Process Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Network Simulator</td>\n",
       "      <td>02:14:46</td>\n",
       "      <td>01:06:44</td>\n",
       "      <td>00:54:21</td>\n",
       "      <td>00:39:40</td>\n",
       "      <td>00:36:09</td>\n",
       "      <td>01:05:59</td>\n",
       "      <td>0.0272GB</td>\n",
       "      <td>0.381GB</td>\n",
       "      <td>0.392GB</td>\n",
       "      <td>00:01:26</td>\n",
       "      <td>17KB</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Optimization Algorithm</td>\n",
       "      <td>00:41:43</td>\n",
       "      <td>00:26:38</td>\n",
       "      <td>00:20:27</td>\n",
       "      <td>00:13:53</td>\n",
       "      <td>00:13:10</td>\n",
       "      <td>00:25:24</td>\n",
       "      <td>0.0276GB</td>\n",
       "      <td>1.37GB</td>\n",
       "      <td>1.5GB</td>\n",
       "      <td>00:02:21</td>\n",
       "      <td>8KB</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DCGAN</td>\n",
       "      <td>01:37:14</td>\n",
       "      <td>01:09:28</td>\n",
       "      <td>00:22:44</td>\n",
       "      <td>00:12:45</td>\n",
       "      <td>00:11:15</td>\n",
       "      <td>01:09:43</td>\n",
       "      <td>1.5476GB</td>\n",
       "      <td>1.87GB</td>\n",
       "      <td>1.9GB</td>\n",
       "      <td>00:04:15</td>\n",
       "      <td>20KB</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RNN</td>\n",
       "      <td>00:17:43</td>\n",
       "      <td>00:12:01</td>\n",
       "      <td>00:07:03</td>\n",
       "      <td>00:05:37</td>\n",
       "      <td>00:04:49</td>\n",
       "      <td>00:12:15</td>\n",
       "      <td>1.2144GB</td>\n",
       "      <td>1.83GB</td>\n",
       "      <td>1.9GB</td>\n",
       "      <td>00:03:36</td>\n",
       "      <td>8KB</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CNN</td>\n",
       "      <td>00:26:04</td>\n",
       "      <td>00:22:22</td>\n",
       "      <td>00:07:07</td>\n",
       "      <td>00:05:24</td>\n",
       "      <td>00:04:47</td>\n",
       "      <td>00:21:18</td>\n",
       "      <td>1.4012GB</td>\n",
       "      <td>1.83GB</td>\n",
       "      <td>1.9GB</td>\n",
       "      <td>00:06:19</td>\n",
       "      <td>4KB</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Job Name Standard Processing Duration PC1  \\\n",
       "0       Network Simulator                         02:14:46   \n",
       "1  Optimization Algorithm                         00:41:43   \n",
       "2                   DCGAN                         01:37:14   \n",
       "3                     RNN                         00:17:43   \n",
       "4                     CNN                         00:26:04   \n",
       "\n",
       "  Standard Processing Duration PC2 Standard Processing Duration PC3  \\\n",
       "0                         01:06:44                         00:54:21   \n",
       "1                         00:26:38                         00:20:27   \n",
       "2                         01:09:28                         00:22:44   \n",
       "3                         00:12:01                         00:07:03   \n",
       "4                         00:22:22                         00:07:07   \n",
       "\n",
       "  Standard Processing Duration PC4 Standard Processing Duration PC5  \\\n",
       "0                         00:39:40                         00:36:09   \n",
       "1                         00:13:53                         00:13:10   \n",
       "2                         00:12:45                         00:11:15   \n",
       "3                         00:05:37                         00:04:49   \n",
       "4                         00:05:24                         00:04:47   \n",
       "\n",
       "  Standard Processing Duration PC6 Required Memory Size For Execution  \\\n",
       "0                         01:05:59                           0.0272GB   \n",
       "1                         00:25:24                           0.0276GB   \n",
       "2                         01:09:43                           1.5476GB   \n",
       "3                         00:12:15                           1.2144GB   \n",
       "4                         00:21:18                           1.4012GB   \n",
       "\n",
       "  Required Disk Size For Execution Docker File Size  \\\n",
       "0                          0.381GB          0.392GB   \n",
       "1                           1.37GB            1.5GB   \n",
       "2                           1.87GB            1.9GB   \n",
       "3                           1.83GB            1.9GB   \n",
       "4                           1.83GB            1.9GB   \n",
       "\n",
       "  Docker File Generation Duration On Master PC Estimated Result File Size  \\\n",
       "0                                     00:01:26                       17KB   \n",
       "1                                     00:02:21                        8KB   \n",
       "2                                     00:04:15                       20KB   \n",
       "3                                     00:03:36                        8KB   \n",
       "4                                     00:06:19                        4KB   \n",
       "\n",
       "   Thread Process Count  \n",
       "0                     1  \n",
       "1                     1  \n",
       "2                    17  \n",
       "3                    17  \n",
       "4                    17  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b758193d-5373-4f50-9149-4677123dd643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_jobs_from_df(df):\n",
    "    jobs = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        # Parse the processing durations for each PC\n",
    "        standard_processing_durations = {\n",
    "            'PC1': parse_time_to_seconds(row['Standard Processing Duration PC1']),\n",
    "            'PC2': parse_time_to_seconds(row['Standard Processing Duration PC2']),\n",
    "            'PC3': parse_time_to_seconds(row['Standard Processing Duration PC3']),\n",
    "            'PC4': parse_time_to_seconds(row['Standard Processing Duration PC4']),\n",
    "            'PC5': parse_time_to_seconds(row['Standard Processing Duration PC5']),\n",
    "            'PC6': parse_time_to_seconds(row['Standard Processing Duration PC6']),\n",
    "        }\n",
    "\n",
    "        # Convert memory, disk sizes, and other attributes\n",
    "        required_memory_size = parse_size(row['Required Memory Size For Execution'])\n",
    "        required_disk_size = parse_size(row['Required Disk Size For Execution'])\n",
    "        docker_file_size = parse_size(row['Docker File Size'])\n",
    "        estimated_result_file_size = parse_size(row['Estimated Result File Size'])\n",
    "        docker_file_gen_duration = parse_time_to_seconds(row['Docker File Generation Duration On Master PC'])\n",
    "\n",
    "        # Create a Job object with the parsed values\n",
    "        job = Job(\n",
    "            ID = index,\n",
    "            name=row['Job Name'],\n",
    "            standard_processing_durations=standard_processing_durations,\n",
    "            required_memory_size_for_execution=required_memory_size,\n",
    "            required_disk_size_for_execution=required_disk_size,\n",
    "            docker_file_size=docker_file_size,\n",
    "            estimated_result_file_size=estimated_result_file_size,\n",
    "            docker_file_generation_duration_on_master_pc=docker_file_gen_duration,\n",
    "            thread_process_count=int(row['Thread Process Count'])\n",
    "        )\n",
    "        \n",
    "        jobs.append(job)\n",
    "    \n",
    "    return jobs\n",
    "\n",
    "# Helper function to parse sizes (e.g., GB, MB, KB) to bytes\n",
    "def parse_size(size_str):\n",
    "    size_str = size_str.lower()\n",
    "    if 'gb' in size_str:\n",
    "        return float(size_str.split('gb')[0].strip()) * 1024 * 1024 * 1024\n",
    "    elif 'mb' in size_str:\n",
    "        return float(size_str.split('mb')[0].strip()) * 1024 * 1024\n",
    "    elif 'kb' in size_str:\n",
    "        return float(size_str.split('kb')[0].strip()) * 1024\n",
    "    elif 'b' in size_str:\n",
    "        return float(size_str.split('b')[0].strip())\n",
    "    return 0.0\n",
    "\n",
    "# Helper function to parse time strings to seconds (e.g., HH:mm:ss)\n",
    "def parse_time_to_seconds(time_str):\n",
    "    time_parts = list(map(int, time_str.split(':')))\n",
    "    return time_parts[0] * 3600 + time_parts[1] * 60 + time_parts[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "23868386-8ef4-4bf6-9378-b5cf691d8659",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++ Job Details ++++++++++\n",
      "ID: 0\n",
      "Name: Network Simulator\n",
      "Standard Processing Duration On PC1: 8086\n",
      "Standard Processing Duration On PC2: 4004\n",
      "Standard Processing Duration On PC3: 3261\n",
      "Standard Processing Duration On PC4: 2380\n",
      "Standard Processing Duration On PC5: 2169\n",
      "Standard Processing Duration On PC6: 3959\n",
      "Required Memory Size For Execution: 29205777.6128\n",
      "Required Disk Size For Execution: 409095634.944\n",
      "Docker File Size: 420906795.008\n",
      "Arrival Time: 0\n",
      "Thread Process Count: 1\n",
      "Estimated Result File Size: 17408.0\n",
      "Docker File Generation Duration On Master PC: 86\n",
      "Current CPU Time: 0.0\n",
      "Currently Assigned To Worker: False\n",
      "Currently Being Processed On Assigned Worker: False\n",
      "Finished Being Processed On Assigned Worker: False\n",
      "++++++++++ End ++++++++++\n",
      "\n",
      "\n",
      "++++++++++ Job Details ++++++++++\n",
      "ID: 1\n",
      "Name: Optimization Algorithm\n",
      "Standard Processing Duration On PC1: 2503\n",
      "Standard Processing Duration On PC2: 1598\n",
      "Standard Processing Duration On PC3: 1227\n",
      "Standard Processing Duration On PC4: 833\n",
      "Standard Processing Duration On PC5: 790\n",
      "Standard Processing Duration On PC6: 1524\n",
      "Required Memory Size For Execution: 29635274.3424\n",
      "Required Disk Size For Execution: 1471026298.88\n",
      "Docker File Size: 1610612736.0\n",
      "Arrival Time: 0\n",
      "Thread Process Count: 1\n",
      "Estimated Result File Size: 8192.0\n",
      "Docker File Generation Duration On Master PC: 141\n",
      "Current CPU Time: 0.0\n",
      "Currently Assigned To Worker: False\n",
      "Currently Being Processed On Assigned Worker: False\n",
      "Finished Being Processed On Assigned Worker: False\n",
      "++++++++++ End ++++++++++\n",
      "\n",
      "\n",
      "++++++++++ Job Details ++++++++++\n",
      "ID: 2\n",
      "Name: DCGAN\n",
      "Standard Processing Duration On PC1: 5834\n",
      "Standard Processing Duration On PC2: 4168\n",
      "Standard Processing Duration On PC3: 1364\n",
      "Standard Processing Duration On PC4: 765\n",
      "Standard Processing Duration On PC5: 675\n",
      "Standard Processing Duration On PC6: 4183\n",
      "Required Memory Size For Execution: 1661722846.8224\n",
      "Required Disk Size For Execution: 2007897210.88\n",
      "Docker File Size: 2040109465.6\n",
      "Arrival Time: 0\n",
      "Thread Process Count: 17\n",
      "Estimated Result File Size: 20480.0\n",
      "Docker File Generation Duration On Master PC: 255\n",
      "Current CPU Time: 0.0\n",
      "Currently Assigned To Worker: False\n",
      "Currently Being Processed On Assigned Worker: False\n",
      "Finished Being Processed On Assigned Worker: False\n",
      "++++++++++ End ++++++++++\n",
      "\n",
      "\n",
      "++++++++++ Job Details ++++++++++\n",
      "ID: 3\n",
      "Name: RNN\n",
      "Standard Processing Duration On PC1: 1063\n",
      "Standard Processing Duration On PC2: 721\n",
      "Standard Processing Duration On PC3: 423\n",
      "Standard Processing Duration On PC4: 337\n",
      "Standard Processing Duration On PC5: 289\n",
      "Standard Processing Duration On PC6: 735\n",
      "Required Memory Size For Execution: 1303952071.0656\n",
      "Required Disk Size For Execution: 1964947537.92\n",
      "Docker File Size: 2040109465.6\n",
      "Arrival Time: 0\n",
      "Thread Process Count: 17\n",
      "Estimated Result File Size: 8192.0\n",
      "Docker File Generation Duration On Master PC: 216\n",
      "Current CPU Time: 0.0\n",
      "Currently Assigned To Worker: False\n",
      "Currently Being Processed On Assigned Worker: False\n",
      "Finished Being Processed On Assigned Worker: False\n",
      "++++++++++ End ++++++++++\n",
      "\n",
      "\n",
      "++++++++++ Job Details ++++++++++\n",
      "ID: 4\n",
      "Name: CNN\n",
      "Standard Processing Duration On PC1: 1564\n",
      "Standard Processing Duration On PC2: 1342\n",
      "Standard Processing Duration On PC3: 427\n",
      "Standard Processing Duration On PC4: 324\n",
      "Standard Processing Duration On PC5: 287\n",
      "Standard Processing Duration On PC6: 1278\n",
      "Required Memory Size For Execution: 1504527043.7888\n",
      "Required Disk Size For Execution: 1964947537.92\n",
      "Docker File Size: 2040109465.6\n",
      "Arrival Time: 0\n",
      "Thread Process Count: 17\n",
      "Estimated Result File Size: 4096.0\n",
      "Docker File Generation Duration On Master PC: 379\n",
      "Current CPU Time: 0.0\n",
      "Currently Assigned To Worker: False\n",
      "Currently Being Processed On Assigned Worker: False\n",
      "Finished Being Processed On Assigned Worker: False\n",
      "++++++++++ End ++++++++++\n",
      "\n",
      "\n",
      "++++++++++ Job Details ++++++++++\n",
      "ID: 5\n",
      "Name: FFmpeg\n",
      "Standard Processing Duration On PC1: 2797\n",
      "Standard Processing Duration On PC2: 1909\n",
      "Standard Processing Duration On PC3: 803\n",
      "Standard Processing Duration On PC4: 479\n",
      "Standard Processing Duration On PC5: 414\n",
      "Standard Processing Duration On PC6: 1831\n",
      "Required Memory Size For Execution: 931578406.5024\n",
      "Required Disk Size For Execution: 2942052597.76\n",
      "Docker File Size: 3006477107.2\n",
      "Arrival Time: 0\n",
      "Thread Process Count: 18\n",
      "Estimated Result File Size: 1825361100.8\n",
      "Docker File Generation Duration On Master PC: 380\n",
      "Current CPU Time: 0.0\n",
      "Currently Assigned To Worker: False\n",
      "Currently Being Processed On Assigned Worker: False\n",
      "Finished Being Processed On Assigned Worker: False\n",
      "++++++++++ End ++++++++++\n",
      "\n",
      "\n",
      "++++++++++ Job Details ++++++++++\n",
      "ID: 6\n",
      "Name: Converter\n",
      "Standard Processing Duration On PC1: 1029\n",
      "Standard Processing Duration On PC2: 694\n",
      "Standard Processing Duration On PC3: 341\n",
      "Standard Processing Duration On PC4: 293\n",
      "Standard Processing Duration On PC5: 255\n",
      "Standard Processing Duration On PC6: 794\n",
      "Required Memory Size For Execution: 811319322.2144\n",
      "Required Disk Size For Execution: 1159641169.92\n",
      "Docker File Size: 1181116006.4\n",
      "Arrival Time: 0\n",
      "Thread Process Count: 1\n",
      "Estimated Result File Size: 188009676.8\n",
      "Docker File Generation Duration On Master PC: 436\n",
      "Current CPU Time: 0.0\n",
      "Currently Assigned To Worker: False\n",
      "Currently Being Processed On Assigned Worker: False\n",
      "Finished Being Processed On Assigned Worker: False\n",
      "++++++++++ End ++++++++++\n",
      "\n",
      "\n",
      "++++++++++ Job Details ++++++++++\n",
      "ID: 7\n",
      "Name: Palabos\n",
      "Standard Processing Duration On PC1: 771\n",
      "Standard Processing Duration On PC2: 518\n",
      "Standard Processing Duration On PC3: 324\n",
      "Standard Processing Duration On PC4: 269\n",
      "Standard Processing Duration On PC5: 199\n",
      "Standard Processing Duration On PC6: 613\n",
      "Required Memory Size For Execution: 986124491.1616\n",
      "Required Disk Size For Execution: 7151120547.84\n",
      "Docker File Size: 7194070220.8\n",
      "Arrival Time: 0\n",
      "Thread Process Count: 2\n",
      "Estimated Result File Size: 256000.0\n",
      "Docker File Generation Duration On Master PC: 877\n",
      "Current CPU Time: 0.0\n",
      "Currently Assigned To Worker: False\n",
      "Currently Being Processed On Assigned Worker: False\n",
      "Finished Being Processed On Assigned Worker: False\n",
      "++++++++++ End ++++++++++\n",
      "\n",
      "\n",
      "++++++++++ Job Details ++++++++++\n",
      "ID: 8\n",
      "Name: Flow\n",
      "Standard Processing Duration On PC1: 1520\n",
      "Standard Processing Duration On PC2: 885\n",
      "Standard Processing Duration On PC3: 668\n",
      "Standard Processing Duration On PC4: 512\n",
      "Standard Processing Duration On PC5: 475\n",
      "Standard Processing Duration On PC6: 862\n",
      "Required Memory Size For Execution: 771805623.0912\n",
      "Required Disk Size For Execution: 461708984.32\n",
      "Docker File Size: 470298918.912\n",
      "Arrival Time: 0\n",
      "Thread Process Count: 4\n",
      "Estimated Result File Size: 0.0\n",
      "Docker File Generation Duration On Master PC: 109\n",
      "Current CPU Time: 0.0\n",
      "Currently Assigned To Worker: False\n",
      "Currently Being Processed On Assigned Worker: False\n",
      "Finished Being Processed On Assigned Worker: False\n",
      "++++++++++ End ++++++++++\n",
      "\n",
      "\n",
      "++++++++++ Job Details ++++++++++\n",
      "ID: 9\n",
      "Name: Network Simulator\n",
      "Standard Processing Duration On PC1: 8086\n",
      "Standard Processing Duration On PC2: 4004\n",
      "Standard Processing Duration On PC3: 3261\n",
      "Standard Processing Duration On PC4: 2380\n",
      "Standard Processing Duration On PC5: 2169\n",
      "Standard Processing Duration On PC6: 3959\n",
      "Required Memory Size For Execution: 29205777.6128\n",
      "Required Disk Size For Execution: 409095634.944\n",
      "Docker File Size: 420906795.008\n",
      "Arrival Time: 0\n",
      "Thread Process Count: 1\n",
      "Estimated Result File Size: 17408.0\n",
      "Docker File Generation Duration On Master PC: 86\n",
      "Current CPU Time: 0.0\n",
      "Currently Assigned To Worker: False\n",
      "Currently Being Processed On Assigned Worker: False\n",
      "Finished Being Processed On Assigned Worker: False\n",
      "++++++++++ End ++++++++++\n",
      "\n",
      "\n",
      "++++++++++ Job Details ++++++++++\n",
      "ID: 10\n",
      "Name: Optimization Algorithm\n",
      "Standard Processing Duration On PC1: 2503\n",
      "Standard Processing Duration On PC2: 1598\n",
      "Standard Processing Duration On PC3: 1227\n",
      "Standard Processing Duration On PC4: 833\n",
      "Standard Processing Duration On PC5: 790\n",
      "Standard Processing Duration On PC6: 1524\n",
      "Required Memory Size For Execution: 29635274.3424\n",
      "Required Disk Size For Execution: 1471026298.88\n",
      "Docker File Size: 1610612736.0\n",
      "Arrival Time: 0\n",
      "Thread Process Count: 1\n",
      "Estimated Result File Size: 8192.0\n",
      "Docker File Generation Duration On Master PC: 141\n",
      "Current CPU Time: 0.0\n",
      "Currently Assigned To Worker: False\n",
      "Currently Being Processed On Assigned Worker: False\n",
      "Finished Being Processed On Assigned Worker: False\n",
      "++++++++++ End ++++++++++\n",
      "\n",
      "\n",
      "++++++++++ Job Details ++++++++++\n",
      "ID: 11\n",
      "Name: DCGAN\n",
      "Standard Processing Duration On PC1: 5834\n",
      "Standard Processing Duration On PC2: 4168\n",
      "Standard Processing Duration On PC3: 1364\n",
      "Standard Processing Duration On PC4: 765\n",
      "Standard Processing Duration On PC5: 675\n",
      "Standard Processing Duration On PC6: 4183\n",
      "Required Memory Size For Execution: 1661722846.8224\n",
      "Required Disk Size For Execution: 2007897210.88\n",
      "Docker File Size: 2040109465.6\n",
      "Arrival Time: 0\n",
      "Thread Process Count: 17\n",
      "Estimated Result File Size: 20480.0\n",
      "Docker File Generation Duration On Master PC: 255\n",
      "Current CPU Time: 0.0\n",
      "Currently Assigned To Worker: False\n",
      "Currently Being Processed On Assigned Worker: False\n",
      "Finished Being Processed On Assigned Worker: False\n",
      "++++++++++ End ++++++++++\n",
      "\n",
      "\n",
      "++++++++++ Job Details ++++++++++\n",
      "ID: 12\n",
      "Name: RNN\n",
      "Standard Processing Duration On PC1: 1063\n",
      "Standard Processing Duration On PC2: 721\n",
      "Standard Processing Duration On PC3: 423\n",
      "Standard Processing Duration On PC4: 337\n",
      "Standard Processing Duration On PC5: 289\n",
      "Standard Processing Duration On PC6: 735\n",
      "Required Memory Size For Execution: 1303952071.0656\n",
      "Required Disk Size For Execution: 1964947537.92\n",
      "Docker File Size: 2040109465.6\n",
      "Arrival Time: 0\n",
      "Thread Process Count: 17\n",
      "Estimated Result File Size: 8192.0\n",
      "Docker File Generation Duration On Master PC: 216\n",
      "Current CPU Time: 0.0\n",
      "Currently Assigned To Worker: False\n",
      "Currently Being Processed On Assigned Worker: False\n",
      "Finished Being Processed On Assigned Worker: False\n",
      "++++++++++ End ++++++++++\n",
      "\n",
      "\n",
      "++++++++++ Job Details ++++++++++\n",
      "ID: 13\n",
      "Name: CNN\n",
      "Standard Processing Duration On PC1: 1564\n",
      "Standard Processing Duration On PC2: 1342\n",
      "Standard Processing Duration On PC3: 427\n",
      "Standard Processing Duration On PC4: 324\n",
      "Standard Processing Duration On PC5: 287\n",
      "Standard Processing Duration On PC6: 1278\n",
      "Required Memory Size For Execution: 1504527043.7888\n",
      "Required Disk Size For Execution: 1964947537.92\n",
      "Docker File Size: 2040109465.6\n",
      "Arrival Time: 0\n",
      "Thread Process Count: 17\n",
      "Estimated Result File Size: 4096.0\n",
      "Docker File Generation Duration On Master PC: 379\n",
      "Current CPU Time: 0.0\n",
      "Currently Assigned To Worker: False\n",
      "Currently Being Processed On Assigned Worker: False\n",
      "Finished Being Processed On Assigned Worker: False\n",
      "++++++++++ End ++++++++++\n",
      "\n",
      "\n",
      "++++++++++ Job Details ++++++++++\n",
      "ID: 14\n",
      "Name: FFmpeg\n",
      "Standard Processing Duration On PC1: 2797\n",
      "Standard Processing Duration On PC2: 1909\n",
      "Standard Processing Duration On PC3: 803\n",
      "Standard Processing Duration On PC4: 479\n",
      "Standard Processing Duration On PC5: 414\n",
      "Standard Processing Duration On PC6: 1831\n",
      "Required Memory Size For Execution: 931578406.5024\n",
      "Required Disk Size For Execution: 2942052597.76\n",
      "Docker File Size: 3006477107.2\n",
      "Arrival Time: 0\n",
      "Thread Process Count: 18\n",
      "Estimated Result File Size: 1825361100.8\n",
      "Docker File Generation Duration On Master PC: 380\n",
      "Current CPU Time: 0.0\n",
      "Currently Assigned To Worker: False\n",
      "Currently Being Processed On Assigned Worker: False\n",
      "Finished Being Processed On Assigned Worker: False\n",
      "++++++++++ End ++++++++++\n",
      "\n",
      "\n",
      "++++++++++ Job Details ++++++++++\n",
      "ID: 15\n",
      "Name: Converter\n",
      "Standard Processing Duration On PC1: 1029\n",
      "Standard Processing Duration On PC2: 694\n",
      "Standard Processing Duration On PC3: 341\n",
      "Standard Processing Duration On PC4: 293\n",
      "Standard Processing Duration On PC5: 255\n",
      "Standard Processing Duration On PC6: 794\n",
      "Required Memory Size For Execution: 811319322.2144\n",
      "Required Disk Size For Execution: 1159641169.92\n",
      "Docker File Size: 1181116006.4\n",
      "Arrival Time: 0\n",
      "Thread Process Count: 1\n",
      "Estimated Result File Size: 188009676.8\n",
      "Docker File Generation Duration On Master PC: 436\n",
      "Current CPU Time: 0.0\n",
      "Currently Assigned To Worker: False\n",
      "Currently Being Processed On Assigned Worker: False\n",
      "Finished Being Processed On Assigned Worker: False\n",
      "++++++++++ End ++++++++++\n",
      "\n",
      "\n",
      "++++++++++ Job Details ++++++++++\n",
      "ID: 16\n",
      "Name: Palabos\n",
      "Standard Processing Duration On PC1: 771\n",
      "Standard Processing Duration On PC2: 518\n",
      "Standard Processing Duration On PC3: 324\n",
      "Standard Processing Duration On PC4: 269\n",
      "Standard Processing Duration On PC5: 199\n",
      "Standard Processing Duration On PC6: 613\n",
      "Required Memory Size For Execution: 986124491.1616\n",
      "Required Disk Size For Execution: 7151120547.84\n",
      "Docker File Size: 7194070220.8\n",
      "Arrival Time: 0\n",
      "Thread Process Count: 2\n",
      "Estimated Result File Size: 256000.0\n",
      "Docker File Generation Duration On Master PC: 877\n",
      "Current CPU Time: 0.0\n",
      "Currently Assigned To Worker: False\n",
      "Currently Being Processed On Assigned Worker: False\n",
      "Finished Being Processed On Assigned Worker: False\n",
      "++++++++++ End ++++++++++\n",
      "\n",
      "\n",
      "++++++++++ Job Details ++++++++++\n",
      "ID: 17\n",
      "Name: Flow\n",
      "Standard Processing Duration On PC1: 1520\n",
      "Standard Processing Duration On PC2: 885\n",
      "Standard Processing Duration On PC3: 668\n",
      "Standard Processing Duration On PC4: 512\n",
      "Standard Processing Duration On PC5: 475\n",
      "Standard Processing Duration On PC6: 862\n",
      "Required Memory Size For Execution: 771805623.0912\n",
      "Required Disk Size For Execution: 461708984.32\n",
      "Docker File Size: 470298918.912\n",
      "Arrival Time: 0\n",
      "Thread Process Count: 4\n",
      "Estimated Result File Size: 0.0\n",
      "Docker File Generation Duration On Master PC: 109\n",
      "Current CPU Time: 0.0\n",
      "Currently Assigned To Worker: False\n",
      "Currently Being Processed On Assigned Worker: False\n",
      "Finished Being Processed On Assigned Worker: False\n",
      "++++++++++ End ++++++++++\n",
      "\n",
      "\n",
      "++++++++++ Job Details ++++++++++\n",
      "ID: 18\n",
      "Name: Network Simulator\n",
      "Standard Processing Duration On PC1: 8086\n",
      "Standard Processing Duration On PC2: 4004\n",
      "Standard Processing Duration On PC3: 3261\n",
      "Standard Processing Duration On PC4: 2380\n",
      "Standard Processing Duration On PC5: 2169\n",
      "Standard Processing Duration On PC6: 3959\n",
      "Required Memory Size For Execution: 29205777.6128\n",
      "Required Disk Size For Execution: 409095634.944\n",
      "Docker File Size: 420906795.008\n",
      "Arrival Time: 0\n",
      "Thread Process Count: 1\n",
      "Estimated Result File Size: 17408.0\n",
      "Docker File Generation Duration On Master PC: 86\n",
      "Current CPU Time: 0.0\n",
      "Currently Assigned To Worker: False\n",
      "Currently Being Processed On Assigned Worker: False\n",
      "Finished Being Processed On Assigned Worker: False\n",
      "++++++++++ End ++++++++++\n",
      "\n",
      "\n",
      "++++++++++ Job Details ++++++++++\n",
      "ID: 19\n",
      "Name: Optimization Algorithm\n",
      "Standard Processing Duration On PC1: 2503\n",
      "Standard Processing Duration On PC2: 1598\n",
      "Standard Processing Duration On PC3: 1227\n",
      "Standard Processing Duration On PC4: 833\n",
      "Standard Processing Duration On PC5: 790\n",
      "Standard Processing Duration On PC6: 1524\n",
      "Required Memory Size For Execution: 29635274.3424\n",
      "Required Disk Size For Execution: 1471026298.88\n",
      "Docker File Size: 1610612736.0\n",
      "Arrival Time: 0\n",
      "Thread Process Count: 1\n",
      "Estimated Result File Size: 8192.0\n",
      "Docker File Generation Duration On Master PC: 141\n",
      "Current CPU Time: 0.0\n",
      "Currently Assigned To Worker: False\n",
      "Currently Being Processed On Assigned Worker: False\n",
      "Finished Being Processed On Assigned Worker: False\n",
      "++++++++++ End ++++++++++\n",
      "\n",
      "\n",
      "++++++++++ Job Details ++++++++++\n",
      "ID: 20\n",
      "Name: DCGAN\n",
      "Standard Processing Duration On PC1: 5834\n",
      "Standard Processing Duration On PC2: 4168\n",
      "Standard Processing Duration On PC3: 1364\n",
      "Standard Processing Duration On PC4: 765\n",
      "Standard Processing Duration On PC5: 675\n",
      "Standard Processing Duration On PC6: 4183\n",
      "Required Memory Size For Execution: 1661722846.8224\n",
      "Required Disk Size For Execution: 2007897210.88\n",
      "Docker File Size: 2040109465.6\n",
      "Arrival Time: 0\n",
      "Thread Process Count: 17\n",
      "Estimated Result File Size: 20480.0\n",
      "Docker File Generation Duration On Master PC: 255\n",
      "Current CPU Time: 0.0\n",
      "Currently Assigned To Worker: False\n",
      "Currently Being Processed On Assigned Worker: False\n",
      "Finished Being Processed On Assigned Worker: False\n",
      "++++++++++ End ++++++++++\n",
      "\n",
      "\n",
      "++++++++++ Job Details ++++++++++\n",
      "ID: 21\n",
      "Name: RNN\n",
      "Standard Processing Duration On PC1: 1063\n",
      "Standard Processing Duration On PC2: 721\n",
      "Standard Processing Duration On PC3: 423\n",
      "Standard Processing Duration On PC4: 337\n",
      "Standard Processing Duration On PC5: 289\n",
      "Standard Processing Duration On PC6: 735\n",
      "Required Memory Size For Execution: 1303952071.0656\n",
      "Required Disk Size For Execution: 1964947537.92\n",
      "Docker File Size: 2040109465.6\n",
      "Arrival Time: 0\n",
      "Thread Process Count: 17\n",
      "Estimated Result File Size: 8192.0\n",
      "Docker File Generation Duration On Master PC: 216\n",
      "Current CPU Time: 0.0\n",
      "Currently Assigned To Worker: False\n",
      "Currently Being Processed On Assigned Worker: False\n",
      "Finished Being Processed On Assigned Worker: False\n",
      "++++++++++ End ++++++++++\n",
      "\n",
      "\n",
      "++++++++++ Job Details ++++++++++\n",
      "ID: 22\n",
      "Name: CNN\n",
      "Standard Processing Duration On PC1: 1564\n",
      "Standard Processing Duration On PC2: 1342\n",
      "Standard Processing Duration On PC3: 427\n",
      "Standard Processing Duration On PC4: 324\n",
      "Standard Processing Duration On PC5: 287\n",
      "Standard Processing Duration On PC6: 1278\n",
      "Required Memory Size For Execution: 1504527043.7888\n",
      "Required Disk Size For Execution: 1964947537.92\n",
      "Docker File Size: 2040109465.6\n",
      "Arrival Time: 0\n",
      "Thread Process Count: 17\n",
      "Estimated Result File Size: 4096.0\n",
      "Docker File Generation Duration On Master PC: 379\n",
      "Current CPU Time: 0.0\n",
      "Currently Assigned To Worker: False\n",
      "Currently Being Processed On Assigned Worker: False\n",
      "Finished Being Processed On Assigned Worker: False\n",
      "++++++++++ End ++++++++++\n",
      "\n",
      "\n",
      "++++++++++ Job Details ++++++++++\n",
      "ID: 23\n",
      "Name: FFmpeg\n",
      "Standard Processing Duration On PC1: 2797\n",
      "Standard Processing Duration On PC2: 1909\n",
      "Standard Processing Duration On PC3: 803\n",
      "Standard Processing Duration On PC4: 479\n",
      "Standard Processing Duration On PC5: 414\n",
      "Standard Processing Duration On PC6: 1831\n",
      "Required Memory Size For Execution: 931578406.5024\n",
      "Required Disk Size For Execution: 2942052597.76\n",
      "Docker File Size: 3006477107.2\n",
      "Arrival Time: 0\n",
      "Thread Process Count: 18\n",
      "Estimated Result File Size: 1825361100.8\n",
      "Docker File Generation Duration On Master PC: 380\n",
      "Current CPU Time: 0.0\n",
      "Currently Assigned To Worker: False\n",
      "Currently Being Processed On Assigned Worker: False\n",
      "Finished Being Processed On Assigned Worker: False\n",
      "++++++++++ End ++++++++++\n",
      "\n",
      "\n",
      "++++++++++ Job Details ++++++++++\n",
      "ID: 24\n",
      "Name: Converter\n",
      "Standard Processing Duration On PC1: 1029\n",
      "Standard Processing Duration On PC2: 694\n",
      "Standard Processing Duration On PC3: 341\n",
      "Standard Processing Duration On PC4: 293\n",
      "Standard Processing Duration On PC5: 255\n",
      "Standard Processing Duration On PC6: 794\n",
      "Required Memory Size For Execution: 811319322.2144\n",
      "Required Disk Size For Execution: 1159641169.92\n",
      "Docker File Size: 1181116006.4\n",
      "Arrival Time: 0\n",
      "Thread Process Count: 1\n",
      "Estimated Result File Size: 188009676.8\n",
      "Docker File Generation Duration On Master PC: 436\n",
      "Current CPU Time: 0.0\n",
      "Currently Assigned To Worker: False\n",
      "Currently Being Processed On Assigned Worker: False\n",
      "Finished Being Processed On Assigned Worker: False\n",
      "++++++++++ End ++++++++++\n",
      "\n",
      "\n",
      "++++++++++ Job Details ++++++++++\n",
      "ID: 25\n",
      "Name: Palabos\n",
      "Standard Processing Duration On PC1: 771\n",
      "Standard Processing Duration On PC2: 518\n",
      "Standard Processing Duration On PC3: 324\n",
      "Standard Processing Duration On PC4: 269\n",
      "Standard Processing Duration On PC5: 199\n",
      "Standard Processing Duration On PC6: 613\n",
      "Required Memory Size For Execution: 986124491.1616\n",
      "Required Disk Size For Execution: 7151120547.84\n",
      "Docker File Size: 7194070220.8\n",
      "Arrival Time: 0\n",
      "Thread Process Count: 2\n",
      "Estimated Result File Size: 256000.0\n",
      "Docker File Generation Duration On Master PC: 877\n",
      "Current CPU Time: 0.0\n",
      "Currently Assigned To Worker: False\n",
      "Currently Being Processed On Assigned Worker: False\n",
      "Finished Being Processed On Assigned Worker: False\n",
      "++++++++++ End ++++++++++\n",
      "\n",
      "\n",
      "++++++++++ Job Details ++++++++++\n",
      "ID: 26\n",
      "Name: Flow\n",
      "Standard Processing Duration On PC1: 1520\n",
      "Standard Processing Duration On PC2: 885\n",
      "Standard Processing Duration On PC3: 668\n",
      "Standard Processing Duration On PC4: 512\n",
      "Standard Processing Duration On PC5: 475\n",
      "Standard Processing Duration On PC6: 862\n",
      "Required Memory Size For Execution: 771805623.0912\n",
      "Required Disk Size For Execution: 461708984.32\n",
      "Docker File Size: 470298918.912\n",
      "Arrival Time: 0\n",
      "Thread Process Count: 4\n",
      "Estimated Result File Size: 0.0\n",
      "Docker File Generation Duration On Master PC: 109\n",
      "Current CPU Time: 0.0\n",
      "Currently Assigned To Worker: False\n",
      "Currently Being Processed On Assigned Worker: False\n",
      "Finished Being Processed On Assigned Worker: False\n",
      "++++++++++ End ++++++++++\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jobs = create_jobs_from_df(jobData)\n",
    "\n",
    "# Printing job details for verification\n",
    "for job in jobs:\n",
    "    job.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e353ce99-ac3f-421e-8c11-9a1ce8511983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_workers_from_df(df):\n",
    "    # List to store Worker objects\n",
    "    workers = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        # Create a new CPU object\n",
    "        cpu_info = CPU(\n",
    "            number_of_cores=int(row['Available CPU Core Number']),\n",
    "            clock_rate_in_hz=convert_to_hz(row['CPU Clock Rate']),\n",
    "            family_name=row['CPU Family Name'].strip(),\n",
    "            denomination=row['CPU Denomination'].strip()\n",
    "        )\n",
    "    \n",
    "        # Create a new Worker object\n",
    "        worker = Worker(\n",
    "            ID=index,\n",
    "            cpu_info=cpu_info,\n",
    "            available_memory_size=convert_to_bytes(row['Available Memory Size']),\n",
    "            available_disk_size=convert_to_bytes(row['Available Disk Size']),\n",
    "            connection_bandwidth_with_master_pc=convert_to_bytes(row['Connection Bandwidth With Master PC']),\n",
    "            connection_delay_with_master_pc=convert_to_seconds(row['Connection Delay With Master PC']),\n",
    "            name=row['Worker PC Name'].strip(),\n",
    "            cpu_usage_in_percentage=0.0,\n",
    "            current_global_cpu_time=0.0\n",
    "        )\n",
    "    \n",
    "        # Add the worker to the list\n",
    "        workers.append(worker)\n",
    "        \n",
    "    return workers\n",
    "    \n",
    "# Function to convert CPU clock rate from various formats to Hz\n",
    "def convert_to_hz(value):\n",
    "    value = value.lower().strip()\n",
    "    if 'ghz' in value:\n",
    "        return float(re.split(r\"ghz\", value)[0].strip()) * 1_000_000_000\n",
    "    elif 'mhz' in value:\n",
    "        return float(re.split(r\"mhz\", value)[0].strip()) * 1_000_000\n",
    "    elif 'khz' in value:\n",
    "        return float(re.split(r\"khz\", value)[0].strip()) * 1_000\n",
    "    elif 'hz' in value:\n",
    "        return float(re.split(r\"hz\", value)[0].strip())\n",
    "    return float(value)\n",
    "\n",
    "# Function to convert memory and disk size to bytes\n",
    "def convert_to_bytes(value):\n",
    "    value = value.lower().strip()\n",
    "    if 'gb' in value:\n",
    "        return float(re.split(r\"gb\", value)[0].strip()) * 1_073_741_824\n",
    "    elif 'mb' in value:\n",
    "        return float(re.split(r\"mb\", value)[0].strip()) * 1_048_576\n",
    "    elif 'kb' in value:\n",
    "        return float(re.split(r\"kb\", value)[0].strip()) * 1_024\n",
    "    elif 'b' in value:\n",
    "        return float(re.split(r\"b\", value)[0].strip())\n",
    "    return float(value)\n",
    "\n",
    "# Function to parse connection delay from time format to seconds\n",
    "def convert_to_seconds(value):\n",
    "    try:\n",
    "        time_obj = datetime.strptime(value.strip(), '%H:%M:%S')\n",
    "        return time_obj.hour * 3600 + time_obj.minute * 60 + time_obj.second\n",
    "    except ValueError:\n",
    "        return float(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7ccee3d2-ad3b-4ca7-9c57-08d74ad5eb63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Worker PC Details **********\n",
      "ID: 0\n",
      "Name: PC1\n",
      "Current Global CPU Time: 0.0\n",
      "CPU Number Of Cores: 4\n",
      "CPU Clock Rate In GHz: 1.7\n",
      "CPU Name: core i3\n",
      "Available Memory Size: 2147483648.0\n",
      "Available Disk Size: 68719476736.0\n",
      "Connection Bandwidth With Master PC: 104857600.0\n",
      "Connection Delay With Master PC: 0\n",
      "CPU Usage In Percentage: 0.0\n",
      "----- Assigned Jobs -----\n",
      "********** End **********\n",
      "\n",
      "\n",
      "********** Worker PC Details **********\n",
      "ID: 1\n",
      "Name: PC2\n",
      "Current Global CPU Time: 0.0\n",
      "CPU Number Of Cores: 4\n",
      "CPU Clock Rate In GHz: 2.6\n",
      "CPU Name: core i5\n",
      "Available Memory Size: 2147483648.0\n",
      "Available Disk Size: 68719476736.0\n",
      "Connection Bandwidth With Master PC: 104857600.0\n",
      "Connection Delay With Master PC: 0\n",
      "CPU Usage In Percentage: 0.0\n",
      "----- Assigned Jobs -----\n",
      "********** End **********\n",
      "\n",
      "\n",
      "********** Worker PC Details **********\n",
      "ID: 2\n",
      "Name: PC3\n",
      "Current Global CPU Time: 0.0\n",
      "CPU Number Of Cores: 8\n",
      "CPU Clock Rate In GHz: 3.4\n",
      "CPU Name: core i7\n",
      "Available Memory Size: 4294967296.0\n",
      "Available Disk Size: 68719476736.0\n",
      "Connection Bandwidth With Master PC: 104857600.0\n",
      "Connection Delay With Master PC: 0\n",
      "CPU Usage In Percentage: 0.0\n",
      "----- Assigned Jobs -----\n",
      "********** End **********\n",
      "\n",
      "\n",
      "********** Worker PC Details **********\n",
      "ID: 3\n",
      "Name: PC4\n",
      "Current Global CPU Time: 0.0\n",
      "CPU Number Of Cores: 16\n",
      "CPU Clock Rate In GHz: 3.6\n",
      "CPU Name: core i9\n",
      "Available Memory Size: 8589934592.0\n",
      "Available Disk Size: 68719476736.0\n",
      "Connection Bandwidth With Master PC: 104857600.0\n",
      "Connection Delay With Master PC: 0\n",
      "CPU Usage In Percentage: 0.0\n",
      "----- Assigned Jobs -----\n",
      "********** End **********\n",
      "\n",
      "\n",
      "********** Worker PC Details **********\n",
      "ID: 4\n",
      "Name: PC5\n",
      "Current Global CPU Time: 0.0\n",
      "CPU Number Of Cores: 20\n",
      "CPU Clock Rate In GHz: 3.7\n",
      "CPU Name: core i9\n",
      "Available Memory Size: 8589934592.0\n",
      "Available Disk Size: 68719476736.0\n",
      "Connection Bandwidth With Master PC: 104857600.0\n",
      "Connection Delay With Master PC: 0\n",
      "CPU Usage In Percentage: 0.0\n",
      "----- Assigned Jobs -----\n",
      "********** End **********\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "workers = create_workers_from_df(workersData)\n",
    "\n",
    "# Print the details of each worker\n",
    "for worker in workers:\n",
    "    worker.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0d3e870f-12c3-4026-ae0e-45d0c69c16d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_matches(jobs,workers,VERBOSE = 0):\n",
    "    #list to create matches\n",
    "    matches = []\n",
    "    \n",
    "    for job in jobs: \n",
    "        for worker in workers:\n",
    "            # Check if the worker meets the conditions to handle the job\n",
    "            if worker.can_handle_job(job):\n",
    "                match = Match(value=(job,worker))\n",
    "                matches.append(match)\n",
    "                if VERBOSE:\n",
    "                    print(f\"{job.name} {worker.name}\")\n",
    "    return matches\n",
    "matches = create_matches(jobs,workers)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7af020f0-6d0c-401a-ba51-fe8ee30e5ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ants = []\n",
    "for i in range(1, len(jobs)+1):\n",
    "    ants.append(Ant(id=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7e73b3fa-a842-46b4-843e-c8cd20587f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No significant improvement for 50 consecutive iterations. Stopping.\n",
      "Converged at iteration 51 with duration difference 0\n"
     ]
    }
   ],
   "source": [
    "optimal_path, total_duration = ACO(jobs, matches, ants, alpha=1, beta=1, evap_coeff=0.9, Q=100, max_iterations=1000, tolerance=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dd48b3cc-1258-44ec-8605-bfef57a74d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker: PC5, Total Duration: 1 hours 36 minutes 25 seconds\n",
      "Worker: PC4, Total Duration: 0 hours 13 minutes 53 seconds\n"
     ]
    }
   ],
   "source": [
    "def display_duration_per_worker(optimal_path):\n",
    "\tworker_jobs = {}\n",
    "\t\n",
    "\tfor job, worker in optimal_path: \n",
    "\t\tif worker not in worker_jobs:\n",
    "\t\t\tworker_jobs[worker] = []\n",
    "\t\tworker_jobs[worker].append(job) \n",
    "\tfor worker, assigned_jobs in worker_jobs.items():\n",
    "\t\tworker_duration = calculate_worker_duration(assigned_jobs, worker)\n",
    "\t\tprint(f\"Worker: {worker.name}, Total Duration: {format_duration(worker_duration)}\")\n",
    "\n",
    "# Example usage after running ACO and getting optimal_path\n",
    "display_duration_per_worker(optimal_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1055ec33-c7b6-4c2f-b5b3-f3f662188ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Path:\n",
      "Job: Network Simulator, Worker: PC5\n",
      "Job: Optimization Algorithm, Worker: PC5\n",
      "Job: DCGAN, Worker: PC5\n",
      "Job: RNN, Worker: PC5\n",
      "Job: CNN, Worker: PC5\n",
      "Job: FFmpeg, Worker: PC5\n",
      "Job: Converter, Worker: PC5\n",
      "Job: Palabos, Worker: PC5\n",
      "Job: Flow, Worker: PC4\n",
      "Job: Network Simulator, Worker: PC5\n",
      "Job: Optimization Algorithm, Worker: PC5\n",
      "Job: DCGAN, Worker: PC5\n",
      "Job: RNN, Worker: PC5\n",
      "Job: CNN, Worker: PC5\n",
      "Job: FFmpeg, Worker: PC5\n",
      "Job: Converter, Worker: PC5\n",
      "Job: Palabos, Worker: PC5\n",
      "Job: Flow, Worker: PC5\n",
      "Job: Network Simulator, Worker: PC5\n",
      "Job: Optimization Algorithm, Worker: PC4\n",
      "Job: DCGAN, Worker: PC5\n",
      "Job: RNN, Worker: PC5\n",
      "Job: CNN, Worker: PC5\n",
      "Job: FFmpeg, Worker: PC5\n",
      "Job: Converter, Worker: PC5\n",
      "Job: Palabos, Worker: PC5\n",
      "Job: Flow, Worker: PC5\n",
      "Total Processing Duration: 1 hours 36 minutes 25 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimal Path:\")\n",
    "for job, worker in optimal_path:\n",
    "    print(f\"Job: {job.name}, Worker: {worker.name}\")\n",
    "print(f\"Total Processing Duration: {format_duration(total_duration)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
