{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb945470-1db4-4215-b34b-2545f84cb404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from copy import deepcopy\n",
    "from threading import Thread\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "816ee1d0-8a5a-4ba9-9293-f86b14e9b3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPU:\n",
    "    def __init__(self, number_of_cores=0, clock_rate_in_hz=0.0, family_name='', denomination=''):\n",
    "        \"\"\"\n",
    "        Initializes a new CPU with the given parameters.\n",
    "        \"\"\"\n",
    "        self.number_of_cores = number_of_cores\n",
    "        self.clock_rate_in_hz = clock_rate_in_hz\n",
    "        self.family_name = family_name\n",
    "        self.denomination = denomination\n",
    "\n",
    "    def duplicate(self):\n",
    "        \"\"\"\n",
    "        Creates and returns a deep copy of this CPU.\n",
    "        \"\"\"\n",
    "        return deepcopy(self)\n",
    "\n",
    "    def outperforms(self, cpu_to_compare_with, multithreaded_program_execution):\n",
    "        \"\"\"\n",
    "        Returns True if this CPU outperforms the one passed in as a parameter for the specified type of program.\n",
    "        \"\"\"\n",
    "\n",
    "        # Priority mapping for CPU families and denominations\n",
    "        cpu_hierarchy = {\n",
    "            'core i9': 4,\n",
    "            'core i7': 3,\n",
    "            'core i5': 2,\n",
    "            'core i3': 1\n",
    "        }\n",
    "\n",
    "        # Determine the CPU type\n",
    "        this_cpu_type = f\"{self.family_name} {self.denomination}\".lower()\n",
    "        other_cpu_type = f\"{cpu_to_compare_with.family_name} {cpu_to_compare_with.denomination}\".lower()\n",
    "\n",
    "        # Compare based on family/denomination hierarchy\n",
    "        this_priority = cpu_hierarchy.get(this_cpu_type, 0)\n",
    "        other_priority = cpu_hierarchy.get(other_cpu_type, 0)\n",
    "\n",
    "        if this_priority != other_priority:\n",
    "            return this_priority > other_priority\n",
    "\n",
    "        # If the same family, compare based on multithread or single-thread performance\n",
    "        if multithreaded_program_execution:\n",
    "            return self.number_of_cores > cpu_to_compare_with.number_of_cores\n",
    "        else:\n",
    "            return self.clock_rate_in_hz > cpu_to_compare_with.clock_rate_in_hz\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        \"\"\"\n",
    "        Compares if two CPU objects are equivalent in terms of performance.\n",
    "        This allows using '==' for CPU objects.\n",
    "        \"\"\"\n",
    "        if not isinstance(other, CPU):\n",
    "            return NotImplemented\n",
    "        return (self.family_name == other.family_name and\n",
    "                self.denomination == other.denomination and\n",
    "                self.number_of_cores == other.number_of_cores and\n",
    "                self.clock_rate_in_hz == other.clock_rate_in_hz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e101d74-8f35-4deb-a672-6927427a3ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker:\n",
    "    def __init__(self, ID=0, cpu_info=None, current_global_cpu_time=0.0, available_memory_size=0.0, \n",
    "                 available_disk_size=0.0, assigned_jobs=None, connection_bandwidth_with_master_pc=0.0, \n",
    "                 connection_delay_with_master_pc=0.0, cpu_usage_in_percentage=0.0, name=\"\", \n",
    "                 original_available_memory_size=0.0, original_available_disk_size=0.0, \n",
    "                 original_connection_bandwidth_with_master_pc=0.0, original_connection_delay_with_master_pc=0.0):\n",
    "        self.ID = ID\n",
    "        self.cpu_info = cpu_info\n",
    "        self.available_memory_size = available_memory_size\n",
    "        self.available_disk_size = available_disk_size\n",
    "        self.connection_bandwidth_with_master_pc = connection_bandwidth_with_master_pc\n",
    "        self.connection_delay_with_master_pc = connection_delay_with_master_pc\n",
    "        self.original_available_memory_size = original_available_memory_size\n",
    "        self.original_available_disk_size = original_available_disk_size\n",
    "        self.original_connection_bandwidth_with_master_pc = original_connection_bandwidth_with_master_pc\n",
    "        self.original_connection_delay_with_master_pc = original_connection_delay_with_master_pc\n",
    "        self.assigned_jobs = assigned_jobs if assigned_jobs is not None else []\n",
    "        self.cpu_usage_in_percentage = cpu_usage_in_percentage\n",
    "        self.name = name\n",
    "        self.current_global_cpu_time = current_global_cpu_time\n",
    "        self.base10_name = 0\n",
    "    def duplicate(self):\n",
    "        return deepcopy(self)\n",
    "\n",
    "    def print(self):\n",
    "        print(\"********** Worker PC Details **********\")\n",
    "        print(f\"ID: {self.ID}\")\n",
    "        print(f\"Name: {self.name}\")\n",
    "        print(f\"Current Global CPU Time: {self.current_global_cpu_time}\")\n",
    "        print(f\"CPU Number Of Cores: {self.cpu_info.number_of_cores}\")\n",
    "        print(f\"CPU Clock Rate In GHz: {self.cpu_info.clock_rate_in_hz / 1_000_000_000}\")\n",
    "        print(f\"CPU Name: {self.cpu_info.family_name} {self.cpu_info.denomination}\")\n",
    "        print(f\"Available Memory Size: {self.available_memory_size}\")\n",
    "        print(f\"Available Disk Size: {self.available_disk_size}\")\n",
    "        print(f\"Connection Bandwidth With Master PC: {self.connection_bandwidth_with_master_pc}\")\n",
    "        print(f\"Connection Delay With Master PC: {self.connection_delay_with_master_pc}\")\n",
    "        print(f\"CPU Usage In Percentage: {self.cpu_usage_in_percentage}\")\n",
    "        print(\"----- Assigned Jobs -----\")\n",
    "        for job in self.assigned_jobs:\n",
    "            print(f\"Job Name: {job.name}\")\n",
    "        print(\"********** End **********\\n\\n\")\n",
    "\n",
    "    def can_handle_job(self, job):\n",
    "        if self.available_memory_size < job.required_memory_size_for_execution:\n",
    "            return False\n",
    "        if self.available_disk_size < job.required_disk_size_for_execution:\n",
    "            return False\n",
    "        if self.cpu_info.number_of_cores < job.thread_process_count:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def reset_resource_usage(self, job):\n",
    "        self.cpu_usage_in_percentage = 0.0\n",
    "        self.available_disk_size += job.required_disk_size_for_execution\n",
    "        self.available_memory_size += job.required_memory_size_for_execution\n",
    "\n",
    "    def execute_job(self, job):\n",
    "        job_execution_thread = Thread(target=job.run)\n",
    "        job_execution_thread.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64662602-70bc-49b1-b4c6-657ad0e51d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Job(Thread):  # Inherit from threading.Thread to make the class runnable\n",
    "    def __init__(self, ID=0, standard_processing_durations=None, required_memory_size_for_execution=0.0, \n",
    "                 required_disk_size_for_execution=0.0, assigned_worker=None, job_current_cpu_time=0.0, \n",
    "                 docker_file_size=0.0, assignment_time=0, estimated_result_file_size=0.0, \n",
    "                 docker_file_generation_duration_on_master_pc=0.0, currently_being_processed_on_assigned_worker=False, \n",
    "                 thread_process_count=1, currently_assigned_to_worker=False, \n",
    "                 finished_being_processed_on_assigned_worker=False, name=\"\", induced_cpu_usage_increase_percentage=0.0):\n",
    "        super().__init__()\n",
    "        self.ID = ID\n",
    "        self.standard_processing_durations = standard_processing_durations or {}\n",
    "        self.required_memory_size_for_execution = required_memory_size_for_execution\n",
    "        self.required_disk_size_for_execution = required_disk_size_for_execution\n",
    "        self.assigned_worker = assigned_worker\n",
    "        self.job_current_cpu_time = job_current_cpu_time\n",
    "        self.docker_file_size = docker_file_size\n",
    "        self.assignment_time = assignment_time\n",
    "        self.estimated_result_file_size = estimated_result_file_size\n",
    "        self.docker_file_generation_duration_on_master_pc = docker_file_generation_duration_on_master_pc\n",
    "        self.currently_being_processed_on_assigned_worker = currently_being_processed_on_assigned_worker\n",
    "        self.thread_process_count = thread_process_count\n",
    "        self.currently_assigned_to_worker = currently_assigned_to_worker\n",
    "        self.finished_being_processed_on_assigned_worker = finished_being_processed_on_assigned_worker\n",
    "        self.name = name\n",
    "        self.induced_cpu_usage_increase_percentage = induced_cpu_usage_increase_percentage\n",
    "\n",
    "    def duplicate(self, job):\n",
    "        return deepcopy(self)\n",
    "\n",
    "    def run(self):\n",
    "        try:\n",
    "            self.currently_being_processed_on_assigned_worker = True\n",
    "            if self.assigned_worker:\n",
    "                print(f\"{self.assigned_worker.name}---<< {self.name} <<---\")\n",
    "\n",
    "                # Simulating processing time based on the assigned worker's CPU info\n",
    "                family_name = self.assigned_worker.cpu_info['family_name']\n",
    "                denomination = self.assigned_worker.cpu_info['denomination']\n",
    "                cores = self.assigned_worker.cpu_info['number_of_cores']\n",
    "                key = f\"{family_name}-{denomination}-{cores}\"\n",
    "\n",
    "                # Sleep for the standard processing duration\n",
    "                if key in self.standard_processing_durations:\n",
    "                    processing_time = self.standard_processing_durations[key]\n",
    "                    time.sleep(processing_time)\n",
    "\n",
    "                # Processing complete\n",
    "                self.finished_being_processed_on_assigned_worker = True\n",
    "                self.currently_being_processed_on_assigned_worker = False\n",
    "                print(f\"{self.assigned_worker.name}--->> {self.name} >>---\")\n",
    "\n",
    "                # Update worker's resources after processing\n",
    "                self.assigned_worker.assigned_jobs.remove(self)\n",
    "                self.assigned_worker.available_disk_size += self.required_disk_size_for_execution\n",
    "                self.assigned_worker.available_memory_size += self.required_memory_size_for_execution\n",
    "                self.assigned_worker.cpu_usage_in_percentage -= self.induced_cpu_usage_increase_percentage\n",
    "\n",
    "                # Reset job's worker assignment\n",
    "                self.currently_assigned_to_worker = False\n",
    "                self.assigned_worker = None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Job execution failed: {e}\")\n",
    "\n",
    "    def print(self):\n",
    "        print(\"++++++++++ Job Details ++++++++++\")\n",
    "        print(f\"ID: {self.ID}\")\n",
    "        print(f\"Name: {self.name}\")\n",
    "        for cpu, duration in self.standard_processing_durations.items():\n",
    "            print(f\"Standard Processing Duration On {cpu}: {duration}\")\n",
    "        print(f\"Required Memory Size For Execution: {self.required_memory_size_for_execution}\")\n",
    "        print(f\"Required Disk Size For Execution: {self.required_disk_size_for_execution}\")\n",
    "        print(f\"Docker File Size: {self.docker_file_size}\")\n",
    "        print(f\"Arrival Time: {self.assignment_time}\")\n",
    "        print(f\"Thread Process Count: {self.thread_process_count}\")\n",
    "        print(f\"Estimated Result File Size: {self.estimated_result_file_size}\")\n",
    "        print(f\"Docker File Generation Duration On Master PC: {self.docker_file_generation_duration_on_master_pc}\")\n",
    "        print(f\"Current CPU Time: {self.job_current_cpu_time}\")\n",
    "        print(f\"Currently Assigned To Worker: {self.currently_assigned_to_worker}\")\n",
    "        print(f\"Currently Being Processed On Assigned Worker: {self.currently_being_processed_on_assigned_worker}\")\n",
    "        print(f\"Finished Being Processed On Assigned Worker: {self.finished_being_processed_on_assigned_worker}\")\n",
    "        if self.assigned_worker:\n",
    "            print(f\"Assigned Worker: {self.assigned_worker.name}\")\n",
    "        print(\"++++++++++ End ++++++++++\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53d67498-8389-4f6f-8619-ff18ca257c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Match:\n",
    "    def __init__(self, value, pheromone):\n",
    "        self.value = value  # value is expected to be a tuple (job, worker)\n",
    "        self.pheromone = pheromone\n",
    "        self.processing_duration = self.calculate_processing_duration()\n",
    "\n",
    "    def calculate_processing_duration(self):\n",
    "        job, worker = self.value\n",
    "        # Use the worker's name (e.g., 'PC1') to get the correct duration from the job's dictionary\n",
    "        return job.standard_processing_durations[worker.name]  # Assume worker has an 'name' or identifier like 'PC1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e06dda2-0719-4a9d-8260-19bebba5611d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ant:\n",
    "    def __init__(self, id, path=[]):\n",
    "        self.id = id\n",
    "        self.path = path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40d92f73-9e31-4f98-8d83-079da32ccc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_paths(ants, jobs, matches, alpha, beta):\n",
    "    for ant in ants:\n",
    "        ant.path = []\n",
    "        for job in jobs:\n",
    "            matching_elements = [match for match in matches if match.value[0] == job]\n",
    "            p = calculate_probabilities(matching_elements, alpha, beta)  # calculate probabilities (worker, probability)\n",
    "            r = random.random()\n",
    "            w = pick_worker(r, p)\n",
    "            ant.path.append((job, w))  # append job-worker pair to the ant's path\n",
    "    return ants\n",
    "\n",
    "def calculate_probabilities(matches, alpha, beta):\n",
    "    \"\"\"\n",
    "    Calculate the probability of selecting each worker for the given job.\n",
    "    \n",
    "    :param matches: List of Match objects where value[0] is the job and value[1] is the worker.\n",
    "    :param alpha: Weight for pheromone importance.\n",
    "    :param beta: Weight for processing duration (distance) importance.\n",
    "    :return: List of tuples (worker, probability).\n",
    "    \"\"\"\n",
    "    desirabilities = []\n",
    "    total_desirability = 0\n",
    "\n",
    "    # Calculate desirability for each match and sum them for normalization\n",
    "    for match in matches:\n",
    "        worker = match.value[1]  # worker is the second item in the tuple (job, worker)\n",
    "        pheromone = match.pheromone\n",
    "        processing_duration = match.processing_duration\n",
    "        proximity = 1 / processing_duration  # Inverse of the distance is the proximity\n",
    "\n",
    "        # Desirability formula: τ_ik^α * η_ik^β\n",
    "        desirability = (pheromone ** alpha) * (proximity ** beta)\n",
    "        desirabilities.append((worker, desirability))\n",
    "        total_desirability += desirability\n",
    "\n",
    "    # Normalize desirabilities to get probabilities\n",
    "    probabilities = [(worker, desirability / total_desirability) for worker, desirability in desirabilities]\n",
    "\n",
    "    return probabilities\n",
    "\n",
    "def pick_worker(random_value, worker_probabilities):\n",
    "    \"\"\"\n",
    "    Select a worker based on the calculated probabilities and a random value.\n",
    "    \n",
    "    :param random_value: Random number between 0 and 1.\n",
    "    :param worker_probabilities: List of tuples (worker, probability).\n",
    "    :return: The selected worker.\n",
    "    \"\"\"\n",
    "    cumulative_prob = 0\n",
    "\n",
    "    for worker, probability in worker_probabilities:\n",
    "        cumulative_prob += probability\n",
    "        if random_value <= cumulative_prob:\n",
    "            return worker  # Return the exact worker when the random value falls within the cumulative probability\n",
    "\n",
    "    return worker_probabilities[-1][0]  # Fallback to return the last worker in case of rounding issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "722a1338-930a-4643-a984-6c0c060a3374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pheromone_update(ants, matches, evap_coeff, Q):\n",
    "    \"\"\"\n",
    "    Updates the pheromone levels on the matches based on the ants' paths.\n",
    "\n",
    "    :param ants: List of ants, where each ant has a path attribute (a list of job-worker tuples).\n",
    "    :param matches: List of Match objects with pheromone levels to be updated.\n",
    "    :param evap_coeff: Coefficient for pheromone evaporation (ρ).\n",
    "    :param Q: Constant value for pheromone deposition.\n",
    "    :return: Updated matches with new pheromone levels.\n",
    "    \"\"\"\n",
    "    # Evaporate pheromones on all matches\n",
    "    for match in matches:\n",
    "        match.pheromone *= evap_coeff  # Apply evaporation to all pheromone values\n",
    "\n",
    "    # Update pheromones based on each ant's path\n",
    "    for ant in ants:\n",
    "        l = max_worker_processing_duration(ant.path)  # Max processing duration across workers\n",
    "        pheromone = Q / l      # Calculate pheromone to deposit (Δτ_ij)\n",
    "\n",
    "        for match in matches:\n",
    "            if match.value in ant.path:  # Check if the job-worker pair was part of the ant's path\n",
    "                match.pheromone += pheromone  # Add pheromone based on the ant's contribution\n",
    "\n",
    "    return matches\n",
    "\n",
    "def max_worker_processing_duration(path):\n",
    "    \"\"\"\n",
    "    Calculates the maximum processing duration for any worker in the ant's path, \n",
    "    considering the worker's ability to run jobs simultaneously based on resource availability.\n",
    "    \n",
    "    :param path: A list of tuples (job, worker) representing the path of an ant.\n",
    "    :return: The maximum processing duration across all workers.\n",
    "    \"\"\"\n",
    "    # Dictionary to store jobs per worker, using worker.name as the key\n",
    "    worker_jobs = {}\n",
    "\n",
    "    # Group jobs by worker\n",
    "    for job, worker in path:\n",
    "        if worker.name not in worker_jobs:\n",
    "            worker_jobs[worker.name] = {'worker': worker, 'jobs': []}  # Store worker object alongside jobs\n",
    "        worker_jobs[worker.name]['jobs'].append(job)\n",
    "\n",
    "    max_duration = 0\n",
    "\n",
    "    # Evaluate the total processing duration per worker\n",
    "    for worker_info in worker_jobs.values():\n",
    "        worker = worker_info['worker']  # Get the worker object\n",
    "        jobs = worker_info['jobs']  # Get the list of jobs\n",
    "        worker_duration = calculate_worker_duration(jobs, worker)\n",
    "        max_duration = max(max_duration, worker_duration)\n",
    "\n",
    "    return max_duration\n",
    "\n",
    "def calculate_worker_duration(jobs, worker):\n",
    "    \"\"\"\n",
    "    Calculates the total duration for a worker considering simultaneous job execution\n",
    "    based on the worker's available resources (memory, disk, CPU cores).\n",
    "    \n",
    "    :param jobs: List of jobs assigned to the worker.\n",
    "    :param worker: The worker object with available resources (memory, disk, cores).\n",
    "    :return: The total time required for the worker to complete all jobs.\n",
    "    \"\"\"\n",
    "    # Sort jobs by their processing duration as a heuristic to allocate resources efficiently\n",
    "    jobs = sorted(jobs, key=lambda job: job.standard_processing_durations[worker.name])\n",
    "\n",
    "    total_time = 0\n",
    "    # Initialize available resources for the worker\n",
    "    available_memory = worker.available_memory_size\n",
    "    available_disk = worker.available_disk_size\n",
    "    available_cores = worker.cpu_info.number_of_cores\n",
    "\n",
    "    # List of jobs that are executing in parallel with their remaining durations\n",
    "    executing_jobs = []\n",
    "\n",
    "    # List to track remaining job processing times\n",
    "    remaining_durations = {job: job.standard_processing_durations[worker.name] for job in jobs}\n",
    "\n",
    "    while jobs or executing_jobs:\n",
    "        # Try to allocate more jobs to be executed in parallel\n",
    "        remaining_jobs = []\n",
    "        for job in jobs:\n",
    "            # Check if the worker can handle this job along with the currently executing jobs\n",
    "            if (available_memory >= job.required_memory_size_for_execution and\n",
    "                available_disk >= job.required_disk_size_for_execution and\n",
    "                available_cores >= job.thread_process_count):\n",
    "                \n",
    "                # If the worker can handle this job, add it to the executing jobs\n",
    "                executing_jobs.append(job)\n",
    "                available_memory -= job.required_memory_size_for_execution\n",
    "                available_disk -= job.required_disk_size_for_execution\n",
    "                available_cores -= job.thread_process_count\n",
    "            else:\n",
    "                # If the worker can't handle it, put it back in the remaining jobs\n",
    "                remaining_jobs.append(job)\n",
    "\n",
    "        # If no jobs are currently executing, we advance time by the duration of the job that finishes next\n",
    "        if executing_jobs:\n",
    "            # Find the job with the shortest remaining processing time in the current executing jobs\n",
    "            min_duration = min(remaining_durations[job] for job in executing_jobs)\n",
    "            total_time += min_duration\n",
    "\n",
    "            # Remove completed jobs and update available resources\n",
    "            completed_jobs = []\n",
    "            for job in executing_jobs:\n",
    "                remaining_durations[job] -= min_duration  # Reduce the remaining duration for this job\n",
    "                if remaining_durations[job] == 0:  # Job has finished\n",
    "                    available_memory += job.required_memory_size_for_execution\n",
    "                    available_disk += job.required_disk_size_for_execution\n",
    "                    available_cores += job.thread_process_count\n",
    "                    completed_jobs.append(job)\n",
    "\n",
    "            # Remove completed jobs from the executing list\n",
    "            executing_jobs = [job for job in executing_jobs if job not in completed_jobs]\n",
    "\n",
    "        # Update the jobs list with the remaining jobs for the next iteration\n",
    "        jobs = remaining_jobs\n",
    "\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4600d4c8-1dce-4a4e-a0f8-a944a2055f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(jobs, matches):\n",
    "    \"\"\"\n",
    "    Evaluates the matches to determine the optimal path with the minimum processing duration.\n",
    "    \n",
    "    :param jobs: List of jobs.\n",
    "    :param matches: List of Match objects (job-worker pairs with pheromone and processing duration).\n",
    "    :return: A tuple containing the optimal path and the total processing duration.\n",
    "             The optimal path is a list of tuples (job, worker), and the total duration is an integer.\n",
    "    \"\"\"\n",
    "    optimal_path = []\n",
    "    worker_jobs = {}  # Dictionary to store jobs assigned to each worker\n",
    "\n",
    "    # Build the optimal path by selecting the match with the highest pheromone level for each job\n",
    "    for job in jobs:\n",
    "        # Filter matches that correspond to the current job\n",
    "        job_matches = [match for match in matches if match.value[0] == job]\n",
    "\n",
    "        # Select the match with the highest pheromone level (most likely assignment)\n",
    "        best_match = max(job_matches, key=lambda m: m.pheromone)\n",
    "\n",
    "        # Add the job-worker pair to the optimal path\n",
    "        worker = best_match.value[1]\n",
    "        optimal_path.append((job, worker))\n",
    "\n",
    "        # Store the job under the worker's list of jobs\n",
    "        if worker not in worker_jobs:\n",
    "            worker_jobs[worker] = []\n",
    "        worker_jobs[worker].append(job)\n",
    "\n",
    "    # Calculate the total processing duration based on the max worker processing duration\n",
    "    total_processing_duration = 0\n",
    "    for worker, assigned_jobs in worker_jobs.items():\n",
    "        worker_duration = calculate_worker_duration(assigned_jobs, worker)\n",
    "        total_processing_duration = max(total_processing_duration, worker_duration)\n",
    "\n",
    "    return optimal_path, total_processing_duration\n",
    "    \n",
    "def format_duration(seconds):\n",
    "    \"\"\"\n",
    "    Converts a duration from seconds to a string in the format of hours, minutes, and seconds.\n",
    "    \n",
    "    :param seconds: The total duration in seconds.\n",
    "    :return: A string representing the duration in 'H hours M minutes S seconds' format.\n",
    "    \"\"\"\n",
    "    hours, remainder = divmod(seconds, 3600)  # Get hours and the remainder seconds\n",
    "    minutes, seconds = divmod(remainder, 60)  # Get minutes and remaining seconds\n",
    "\n",
    "    # Build the formatted string\n",
    "    duration_str = f\"{int(hours)} hours {int(minutes)} minutes {int(seconds)} seconds\"\n",
    "    \n",
    "    return duration_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1db159ab-c99d-4bc0-b0e3-db25c81c4769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ACO(jobs, matches, ants, alpha, beta, evap_coeff, Q, max_iterations=1000, tolerance=1e-5, patience=500):\n",
    "    \"\"\"\n",
    "    Runs the Ant Colony Optimization (ACO) algorithm to find the optimal job-worker assignments.\n",
    "    \n",
    "    :param jobs: List of jobs.\n",
    "    :param matches: List of Match objects (job-worker pairs with pheromone and processing duration).\n",
    "    :param ants: List of Ant objects, where each ant has a path.\n",
    "    :param alpha: Pheromone influence factor.\n",
    "    :param beta: Heuristic information influence factor.\n",
    "    :param evap_coeff: Coefficient for pheromone evaporation (ρ).\n",
    "    :param Q: Constant value for pheromone deposition.\n",
    "    :param max_iterations: Maximum number of iterations to run.\n",
    "    :param tolerance: Threshold to stop the algorithm when the change in total duration is small enough.\n",
    "    :param patience: Number of consecutive iterations without significant improvement before stopping.\n",
    "    :return: A tuple containing the optimal path and the total processing duration.\n",
    "    \"\"\"\n",
    "    previous_duration = float('inf')  # Start with an infinitely large duration\n",
    "    iteration = 0\n",
    "    no_improvement_count = 0  # Counter for iterations without significant improvement\n",
    "\n",
    "    while iteration < max_iterations:\n",
    "        # Generate paths for ants based on current pheromone levels\n",
    "        ants = generate_paths(ants, jobs, matches, alpha, beta)\n",
    "\n",
    "        # Update pheromone levels based on the paths taken by the ants\n",
    "        matches = pheromone_update(ants, matches, evap_coeff, Q)\n",
    "\n",
    "        # Evaluate the current best path and its total duration\n",
    "        optimal_path, total_duration = evaluate(jobs, matches)\n",
    "\n",
    "        # Calculate the difference in total duration between iterations\n",
    "        duration_difference = abs(previous_duration - total_duration)\n",
    "\n",
    "        # Check if the difference is below the tolerance (convergence criteria)\n",
    "        if duration_difference < tolerance:\n",
    "            no_improvement_count += 1\n",
    "        else:\n",
    "            no_improvement_count = 0  # Reset the count if there's significant improvement\n",
    "\n",
    "        # If there has been no significant improvement for a certain number of iterations, break\n",
    "        if no_improvement_count >= patience:\n",
    "            print(f\"No significant improvement for {patience} consecutive iterations. Stopping.\")\n",
    "            print(f\"Converged at iteration {iteration} with duration difference {duration_difference}\")\n",
    "            break\n",
    "\n",
    "        # If the difference is below the tolerance and we have met the patience criteria, stop\n",
    "        # if no_improvement_count == 0 and duration_difference < tolerance:\n",
    "        #     print(f\"Converged at iteration {iteration} with duration difference {duration_difference}\")\n",
    "        #     break\n",
    "\n",
    "        # Update previous duration and increment iteration counter\n",
    "        previous_duration = total_duration\n",
    "        iteration += 1\n",
    "\n",
    "    # Return the best path found and its total processing duration\n",
    "    print(total_duration)\n",
    "    return optimal_path, total_duration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f316a00-c4b0-414c-9530-9bf0adb89e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobData = pd.read_csv(\"data/jobs36.csv\")\n",
    "workersData = pd.read_csv(\"data/workers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fba0ed0d-5fd7-44db-b13f-d99de404a793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jobData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b758193d-5373-4f50-9149-4677123dd643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_jobs_from_df(df):\n",
    "    jobs = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        # Parse the processing durations for each PC\n",
    "        standard_processing_durations = {\n",
    "            'PC1': parse_time_to_seconds(row['Standard Processing Duration PC1']),\n",
    "            'PC2': parse_time_to_seconds(row['Standard Processing Duration PC2']),\n",
    "            'PC3': parse_time_to_seconds(row['Standard Processing Duration PC3']),\n",
    "            'PC4': parse_time_to_seconds(row['Standard Processing Duration PC4']),\n",
    "            'PC5': parse_time_to_seconds(row['Standard Processing Duration PC5']),\n",
    "            'PC6': parse_time_to_seconds(row['Standard Processing Duration PC6']),\n",
    "        }\n",
    "\n",
    "        # Convert memory, disk sizes, and other attributes\n",
    "        required_memory_size = parse_size(row['Required Memory Size For Execution'])\n",
    "        required_disk_size = parse_size(row['Required Disk Size For Execution'])\n",
    "        docker_file_size = parse_size(row['Docker File Size'])\n",
    "        estimated_result_file_size = parse_size(row['Estimated Result File Size'])\n",
    "        docker_file_gen_duration = parse_time_to_seconds(row['Docker File Generation Duration On Master PC'])\n",
    "\n",
    "        # Create a Job object with the parsed values\n",
    "        job = Job(\n",
    "            ID = index,\n",
    "            name=row['Job Name'],\n",
    "            standard_processing_durations=standard_processing_durations,\n",
    "            required_memory_size_for_execution=required_memory_size,\n",
    "            required_disk_size_for_execution=required_disk_size,\n",
    "            docker_file_size=docker_file_size,\n",
    "            estimated_result_file_size=estimated_result_file_size,\n",
    "            docker_file_generation_duration_on_master_pc=docker_file_gen_duration,\n",
    "            thread_process_count=int(row['Thread Process Count'])\n",
    "        )\n",
    "        \n",
    "        jobs.append(job)\n",
    "    \n",
    "    return jobs\n",
    "\n",
    "# Helper function to parse sizes (e.g., GB, MB, KB) to bytes\n",
    "def parse_size(size_str):\n",
    "    size_str = size_str.lower()\n",
    "    if 'gb' in size_str:\n",
    "        return float(size_str.split('gb')[0].strip()) * 1024 * 1024 * 1024\n",
    "    elif 'mb' in size_str:\n",
    "        return float(size_str.split('mb')[0].strip()) * 1024 * 1024\n",
    "    elif 'kb' in size_str:\n",
    "        return float(size_str.split('kb')[0].strip()) * 1024\n",
    "    elif 'b' in size_str:\n",
    "        return float(size_str.split('b')[0].strip())\n",
    "    return 0.0\n",
    "\n",
    "# Helper function to parse time strings to seconds (e.g., HH:mm:ss)\n",
    "def parse_time_to_seconds(time_str):\n",
    "    time_parts = list(map(int, time_str.split(':')))\n",
    "    return time_parts[0] * 3600 + time_parts[1] * 60 + time_parts[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23868386-8ef4-4bf6-9378-b5cf691d8659",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jobs = create_jobs_from_df(jobData)\n",
    "\n",
    "# Printing job details for verification\n",
    "# for job in jobs:\n",
    "#     job.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e353ce99-ac3f-421e-8c11-9a1ce8511983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_workers_from_df(df):\n",
    "    # List to store Worker objects\n",
    "    workers = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        # Create a new CPU object\n",
    "        cpu_info = CPU(\n",
    "            number_of_cores=int(row['Available CPU Core Number']),\n",
    "            clock_rate_in_hz=convert_to_hz(row['CPU Clock Rate']),\n",
    "            family_name=row['CPU Family Name'].strip(),\n",
    "            denomination=row['CPU Denomination'].strip()\n",
    "        )\n",
    "    \n",
    "        # Create a new Worker object\n",
    "        worker = Worker(\n",
    "            ID=index,\n",
    "            cpu_info=cpu_info,\n",
    "            available_memory_size=convert_to_bytes(row['Available Memory Size']),\n",
    "            available_disk_size=convert_to_bytes(row['Available Disk Size']),\n",
    "            connection_bandwidth_with_master_pc=convert_to_bytes(row['Connection Bandwidth With Master PC']),\n",
    "            connection_delay_with_master_pc=convert_to_seconds(row['Connection Delay With Master PC']),\n",
    "            name=row['Worker PC Name'].strip(),\n",
    "            cpu_usage_in_percentage=0.0,\n",
    "            current_global_cpu_time=0.0\n",
    "        )\n",
    "    \n",
    "        # Add the worker to the list\n",
    "        workers.append(worker)\n",
    "        \n",
    "    return workers\n",
    "    \n",
    "# Function to convert CPU clock rate from various formats to Hz\n",
    "def convert_to_hz(value):\n",
    "    value = value.lower().strip()\n",
    "    if 'ghz' in value:\n",
    "        return float(re.split(r\"ghz\", value)[0].strip()) * 1_000_000_000\n",
    "    elif 'mhz' in value:\n",
    "        return float(re.split(r\"mhz\", value)[0].strip()) * 1_000_000\n",
    "    elif 'khz' in value:\n",
    "        return float(re.split(r\"khz\", value)[0].strip()) * 1_000\n",
    "    elif 'hz' in value:\n",
    "        return float(re.split(r\"hz\", value)[0].strip())\n",
    "    return float(value)\n",
    "\n",
    "# Function to convert memory and disk size to bytes\n",
    "def convert_to_bytes(value):\n",
    "    value = value.lower().strip()\n",
    "    if 'gb' in value:\n",
    "        return float(re.split(r\"gb\", value)[0].strip()) * 1_073_741_824\n",
    "    elif 'mb' in value:\n",
    "        return float(re.split(r\"mb\", value)[0].strip()) * 1_048_576\n",
    "    elif 'kb' in value:\n",
    "        return float(re.split(r\"kb\", value)[0].strip()) * 1_024\n",
    "    elif 'b' in value:\n",
    "        return float(re.split(r\"b\", value)[0].strip())\n",
    "    return float(value)\n",
    "\n",
    "# Function to parse connection delay from time format to seconds\n",
    "def convert_to_seconds(value):\n",
    "    try:\n",
    "        time_obj = datetime.strptime(value.strip(), '%H:%M:%S')\n",
    "        return time_obj.hour * 3600 + time_obj.minute * 60 + time_obj.second\n",
    "    except ValueError:\n",
    "        return float(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ccee3d2-ad3b-4ca7-9c57-08d74ad5eb63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "workers = create_workers_from_df(workersData)\n",
    "\n",
    "# Print the details of each worker\n",
    "# for worker in workers:\n",
    "#     worker.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d3e870f-12c3-4026-ae0e-45d0c69c16d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_matches(jobs,workers,pheromone,VERBOSE = 0):\n",
    "    #list to create matches\n",
    "    matches = []\n",
    "    \n",
    "    for job in jobs: \n",
    "        for worker in workers:\n",
    "            # Check if the worker meets the conditions to handle the job\n",
    "            if worker.can_handle_job(job):\n",
    "                match = Match(value=(job,worker),pheromone=pheromone)\n",
    "                matches.append(match)\n",
    "                if VERBOSE:\n",
    "                    print(f\"{job.name} {worker.name}\")\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7af020f0-6d0c-401a-ba51-fe8ee30e5ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = create_matches(jobs,workers,0.1)    \n",
    "ants = []\n",
    "for i in range(1, 100*len(jobs)+1):\n",
    "    ants.append(Ant(id=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e73b3fa-a842-46b4-843e-c8cd20587f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No significant improvement for 500 consecutive iterations. Stopping.\n",
      "Converged at iteration 500 with duration difference 0\n",
      "9304\n"
     ]
    }
   ],
   "source": [
    "optimal_path, total_duration = ACO(jobs, matches, ants, alpha=3.875, beta=1, evap_coeff=0.5, Q=len(jobs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c846743-ee0f-4648-90b6-45bc55172d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define dataset paths\n",
    "# job_datasets = [f\"data/jobs{i}.csv\" for i in range(36, 91, 9)]\n",
    "\n",
    "# results = []\n",
    "\n",
    "# for job_path in job_datasets:\n",
    "#     # Load jobs data for each dataset\n",
    "#     job_data = pd.read_csv(job_path)\n",
    "#     jobs = create_jobs_from_df(job_data)\n",
    "\n",
    "#     # Generate matches and initialize ants\n",
    "#     matches = create_matches(jobs, workers, pheromone=0.1)\n",
    "#     ants = [Ant(id=i) for i in range(1, 2 * len(jobs) + 1)]\n",
    "\n",
    "#     # Run ACO\n",
    "#     optimal_path, total_duration = ACO(jobs, matches, ants, alpha=3.875, beta=1, evap_coeff=0.7, Q=len(jobs))\n",
    "\n",
    "#     # Store the result with path and duration\n",
    "#     results.append({\n",
    "#     \"job_dataset\": job_path,\n",
    "#     \"optimal_path\": [(job.name, worker.name) for job, worker in optimal_path],\n",
    "#     \"total_duration\": total_duration\n",
    "#     })\n",
    "\n",
    "# # Display results sorted by total duration\n",
    "# for result in sorted(results, key=lambda x: x[\"total_duration\"]):\n",
    "#     print(f\"Dataset: {result['job_dataset']}\")\n",
    "#     print(f\"Optimal Path: {result['optimal_path']}\")\n",
    "#     print(f\"Total Duration: {format_duration(result['total_duration'])} hours, minutes, and seconds\")  # format as needed\n",
    "#     print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c29d517c-e6fe-4368-80df-b46a4fab3060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_optimal_path(jobs, matches, ants, alpha=1, beta=9, evap_coeff=0.2, Q=len(jobs)/4, iterations=100):\n",
    "#     \"\"\"\n",
    "#     Executes the ACO function multiple times to find the best path with the lowest total duration.\n",
    "    \n",
    "#     :param jobs: List of jobs.\n",
    "#     :param matches: List of Match objects.\n",
    "#     :param ants: List of Ant objects.\n",
    "#     :param alpha: Pheromone importance.\n",
    "#     :param beta: Heuristic importance.\n",
    "#     :param evap_coeff: Pheromone evaporation coefficient.\n",
    "#     :param Q: Constant for pheromone deposition.\n",
    "#     :param iterations: Number of times to run the ACO function.\n",
    "#     :return: A tuple containing the optimal path and the lowest total duration found.\n",
    "#     \"\"\"\n",
    "#     best_path = None\n",
    "#     lowest_duration = float('inf')\n",
    "\n",
    "#     for i in range(iterations):\n",
    "#         # Run the ACO algorithm\n",
    "#         optimal_path, total_duration = ACO(jobs, matches, ants, alpha, beta, evap_coeff, Q)\n",
    "\n",
    "#         # Check if the current run's duration is the lowest\n",
    "#         if total_duration < lowest_duration:\n",
    "#             lowest_duration = total_duration\n",
    "#             best_path = optimal_path\n",
    "        \n",
    "#         print(f\"Iteration {i+1}: Duration={total_duration}, Best Duration={lowest_duration}\")\n",
    "\n",
    "#         time.sleep(1)\n",
    "\n",
    "#     return best_path, lowest_duration\n",
    "\n",
    "# # Usage:\n",
    "# best_path, lowest_duration = find_optimal_path(jobs, matches, ants, alpha=3.875, beta=1, evap_coeff=0.7, Q=len(jobs), iterations=50)\n",
    "\n",
    "# print(\"Optimal Path:\", [(job.name, worker.name) for job, worker in best_path])\n",
    "# print(\"Lowest Total Duration:\", lowest_duration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd48b3cc-1258-44ec-8605-bfef57a74d25",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \t\t\u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWorker: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mworker\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Total Duration: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mformat_duration(worker_duration)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Example usage after running ACO and getting optimal_path\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m display_duration_per_worker(\u001b[43mbest_path\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best_path' is not defined"
     ]
    }
   ],
   "source": [
    "def display_duration_per_worker(optimal_path):\n",
    "\tworker_jobs = {}\n",
    "\t\n",
    "\tfor job, worker in optimal_path: \n",
    "\t\tif worker not in worker_jobs:\n",
    "\t\t\tworker_jobs[worker] = []\n",
    "\t\tworker_jobs[worker].append(job) \n",
    "\tfor worker, assigned_jobs in worker_jobs.items():\n",
    "\t\tworker_duration = calculate_worker_duration(assigned_jobs, worker)\n",
    "\t\tprint(f\"Worker: {worker.name}, Total Duration: {format_duration(worker_duration)}\")\n",
    "\n",
    "# Example usage after running ACO and getting optimal_path\n",
    "display_duration_per_worker(best_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1055ec33-c7b6-4c2f-b5b3-f3f662188ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimal Path:\")\n",
    "for job, worker in best_path:\n",
    "    print(f\"Job: {job.name}, Worker: {worker.name}\")\n",
    "print(f\"Total Processing Duration: {format_duration(lowest_duration)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21a6fb3-1e69-4229-8a56-fc7adb792e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import product\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def fine_tune_ACO(jobs, workers, initial_pheromones, num_ants_list, alpha_values, beta_values, evap_coeffs, Q_values, max_iterations=1000, tolerance=1e-5):\n",
    "#     \"\"\"\n",
    "#     Fine-tunes the ACO algorithm by testing all combinations of the given parameter values.\n",
    "    \n",
    "#     :param jobs: List of job objects.\n",
    "#     :param workers: List of worker objects.\n",
    "#     :param initial_pheromones: List of initial pheromone levels to test.\n",
    "#     :param num_ants_list: List of numbers of ants to test.\n",
    "#     :param alpha_values: List of alpha values to test.\n",
    "#     :param beta_values: List of beta values to test.\n",
    "#     :param evap_coeffs: List of evaporation coefficients to test.\n",
    "#     :param Q_values: List of Q values to test.\n",
    "#     :param max_iterations: Maximum number of iterations for each ACO run.\n",
    "#     :param tolerance: Convergence tolerance for each ACO run.\n",
    "#     :return: Sorted list of results with total duration and corresponding parameters.\n",
    "#     \"\"\"\n",
    "#     # Initialize a list to store results\n",
    "#     results = []\n",
    "    \n",
    "#     # Calculate the total number of combinations for tqdm\n",
    "#     total_combinations = len(initial_pheromones) * len(num_ants_list) * len(alpha_values) * len(beta_values) * len(evap_coeffs) * len(Q_values)\n",
    "\n",
    "#     # Generate all combinations of the parameter values\n",
    "#     for initial_pheromone, num_ants, alpha, beta, evap_coeff, Q in tqdm(product(initial_pheromones, num_ants_list, alpha_values, beta_values, evap_coeffs, Q_values), total=total_combinations, desc=\"Fine-tuning parameters\"):\n",
    "        \n",
    "#         # Create matches for the current initial pheromone level\n",
    "#         matches = create_matches(jobs, workers, initial_pheromone)\n",
    "        \n",
    "#         # Create ants for the current number of ants\n",
    "#         ants = [Ant(id=i) for i in range(1, num_ants + 1)]\n",
    "        \n",
    "#         # Run the ACO algorithm with the current combination of parameters\n",
    "#         optimal_path, total_duration = ACO(jobs, matches, ants, alpha, beta, evap_coeff, Q, max_iterations, tolerance, max_iterations/2)\n",
    "        \n",
    "#         # Store the result with parameters for sorting later\n",
    "#         results.append({\n",
    "#             'total_duration': total_duration,\n",
    "#             'parameters': {\n",
    "#                 'initial_pheromone': initial_pheromone,\n",
    "#                 'num_ants': num_ants,\n",
    "#                 'alpha': alpha,\n",
    "#                 'beta': beta,\n",
    "#                 'evap_coeff': evap_coeff,\n",
    "#                 'Q': Q\n",
    "#             }\n",
    "#         })\n",
    "\n",
    "#     # Sort results by total duration in ascending order\n",
    "#     results.sort(key=lambda x: x['total_duration'])\n",
    "\n",
    "#     # Print sorted results\n",
    "#     for result in results:\n",
    "#         params = result['parameters']\n",
    "#         print(f\"Total Duration: {result['total_duration']} | Parameters: Initial Pheromone: {params['initial_pheromone']}, \"\n",
    "#               f\"Num Ants: {params['num_ants']}, Alpha: {params['alpha']}, Beta: {params['beta']}, \"\n",
    "#               f\"Evap Coeff: {params['evap_coeff']}, Q: {params['Q']}\")\n",
    "\n",
    "#     return results\n",
    "\n",
    "# # Define the parameters based on provided ranges and scaling method\n",
    "# def define_parameter_sets(jobs, workers):\n",
    "#     # Number of jobs (nodes) used to scale Q\n",
    "#     N = len(jobs)\n",
    "    \n",
    "#     # Define parameter sets\n",
    "#     initial_pheromones = np.linspace(0.1, 1.0, 5).tolist()  # Initial pheromone level τ₀ from 0.1 to 1.0\n",
    "#     num_ants_list = [int(x) for x in np.linspace(N, 2 * N, 10).tolist()]  # Number of ants from N to 2N\n",
    "#     alpha_values = np.linspace(0.5, 5, 5).tolist()  # Alpha (α) values from 0.5 to 5\n",
    "#     beta_values = np.linspace(1, 10, 5).tolist()  # Beta (β) values from 1 to 10\n",
    "#     evap_coeffs = np.linspace(0.1, 0.9, 5).tolist()  # Evaporation coefficient (ρ) from 0.1 to 0.9\n",
    "\n",
    "#     # Q values scaled by path length (N) with k ranging from 1 to 5\n",
    "#     Q_values = [k * N for k in range(1, 6)]\n",
    "\n",
    "#     # Combine parameter sets for fine-tuning\n",
    "#     return initial_pheromones, num_ants_list, alpha_values, beta_values, evap_coeffs, Q_values\n",
    "\n",
    "# # Usage to retrieve parameter sets\n",
    "# initial_pheromones, num_ants_list, alpha_values, beta_values, evap_coeffs, Q_values = define_parameter_sets(jobs, workers)\n",
    "# print(\"Initial Pheromones:\", initial_pheromones)\n",
    "# print(\"Number of Ants:\", num_ants_list)\n",
    "# print(\"Alpha Values:\", alpha_values)\n",
    "# print(\"Beta Values:\", beta_values)\n",
    "# print(\"Evaporation Coefficients:\", evap_coeffs)\n",
    "# print(\"Q Values:\", Q_values)\n",
    "\n",
    "# # Run the fine-tuning function\n",
    "# fine_tune_ACO(jobs, workers, initial_pheromones, num_ants_list, alpha_values, beta_values, evap_coeffs, Q_values)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
